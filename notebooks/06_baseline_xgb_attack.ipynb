{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397e678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS\n",
      "Using processed data from: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# --------------------------------------------------------------\n",
    "# 06_baseline_xgb_attack.py\n",
    "# Baseline: Single-task XGBoost for attack_id prediction\n",
    "# Dataset: CIC IoT-IDAD 2024 (packet-based processed subset)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ==============================================================\n",
    "# 1. Imports,  Paths, Configurations, Constants\n",
    "# ==============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[0]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "TRAIN_PATH = PROCESSED_DIR / \"packets_train.csv\"\n",
    "VAL_PATH   = PROCESSED_DIR / \"packets_val.csv\"\n",
    "TEST_PATH  = PROCESSED_DIR / \"packets_test.csv\"\n",
    "ATTACK_LABEL_MAP_PATH = PROCESSED_DIR / \"attack_label_mapping.json\"\n",
    "\n",
    "TARGET_COL = \"attack_id\"\n",
    "SUBSAMPLE_N = 300_000  # XGB is faster than RF for large N\n",
    "\n",
    "XGB_CONFIG = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"tree_method\": \"hist\",       # use 'gpu_hist' if GPU is available\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Using processed data from:\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9354e43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2126280, 139)\n",
      "Val   shape: (455632, 139)\n",
      "Test  shape: (455632, 139)\n",
      "Number of attack classes: 8\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 2. Load Processed Data\n",
    "# ==============================================================\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df   = pd.read_csv(VAL_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Val   shape:\", val_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "\n",
    "with open(ATTACK_LABEL_MAP_PATH, \"r\") as f:\n",
    "    attack_label_mapping = json.load(f)[\"id_to_attack\"]\n",
    "\n",
    "num_attacks = len(attack_label_mapping)\n",
    "print(\"Number of attack classes:\", num_attacks)\n",
    "\n",
    "# update num_class from mapping\n",
    "XGB_CONFIG[\"num_class\"] = num_attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14ef08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of feature columns: 119\n",
      "Example features: ['stream', 'src_port', 'dst_port', 'inter_arrival_time', 'time_since_previously_displayed_frame', 'port_class_dst', 'l4_tcp', 'l4_udp', 'ttl', 'eth_size', 'tcp_window_size', 'payload_entropy', 'handshake_cipher_suites_length', 'handshake_ciphersuites', 'handshake_extensions_length']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 3. Feature Selection (Numeric Only)\n",
    "# ==============================================================\n",
    "\n",
    "numeric_cols = train_df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c not in [TARGET_COL, \"device_id\"]]\n",
    "\n",
    "print(\"\\nNumber of feature columns:\", len(feature_cols))\n",
    "print(\"Example features:\", feature_cols[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21479a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning NaN/Inf...\n",
      "  [train] NaN before fill: 20652772, filling with 0.\n",
      "  [val] NaN before fill: 4432636, filling with 0.\n",
      "  [test] NaN before fill: 4426542, filling with 0.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 4. Cleaning: NaN / Inf Handling\n",
    "# ==============================================================\n",
    "\n",
    "def clean_df(df: pd.DataFrame, feature_cols, name: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    nan_before = df[feature_cols].isna().sum().sum()\n",
    "    if nan_before > 0:\n",
    "        print(f\"  [{name}] NaN before fill: {nan_before}, filling with 0.\")\n",
    "        df[feature_cols] = df[feature_cols].fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"\\nCleaning NaN/Inf...\")\n",
    "train_df = clean_df(train_df, feature_cols, \"train\")\n",
    "val_df   = clean_df(val_df, feature_cols, \"val\")\n",
    "test_df  = clean_df(test_df, feature_cols, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b754f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardising features (z-score on train stats)...\n",
      "  [train] NaN after std: 0, Inf: 0\n",
      "  [val] NaN after std: 0, Inf: 0\n",
      "  [test] NaN after std: 0, Inf: 0\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 5. Standardisation (Z-score)\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\nStandardising features (z-score on train stats)...\")\n",
    "means = train_df[feature_cols].mean()\n",
    "stds  = train_df[feature_cols].std().replace(0, 1.0)\n",
    "\n",
    "def standardise(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[feature_cols] = (df[feature_cols] - means) / stds\n",
    "    df[feature_cols] = df[feature_cols].clip(-10, 10)\n",
    "    n_nan = df[feature_cols].isna().sum().sum()\n",
    "    n_inf = np.isinf(df[feature_cols].values).sum()\n",
    "    print(f\"  [{name}] NaN after std: {n_nan}, Inf: {n_inf}\")\n",
    "    if n_nan > 0 or n_inf > 0:\n",
    "        raise ValueError(f\"Found NaN/Inf in {name} after standardisation.\")\n",
    "    return df\n",
    "\n",
    "train_df = standardise(train_df, \"train\")\n",
    "val_df   = standardise(val_df, \"val\")\n",
    "test_df  = standardise(test_df, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e728471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full training size: 2126280\n",
      "Subsampled training size for XGBoost: 300000\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 6. Numpy Arrays + Optional Subsampling\n",
    "# ==============================================================\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df[TARGET_COL].values\n",
    "\n",
    "X_val   = val_df[feature_cols].values\n",
    "y_val   = val_df[TARGET_COL].values\n",
    "\n",
    "X_test  = test_df[feature_cols].values\n",
    "y_test  = test_df[TARGET_COL].values\n",
    "\n",
    "print(\"\\nFull training size:\", X_train.shape[0])\n",
    "\n",
    "if SUBSAMPLE_N is not None and X_train.shape[0] > SUBSAMPLE_N:\n",
    "    idx = np.random.choice(X_train.shape[0], size=SUBSAMPLE_N, replace=False)\n",
    "    X_train_sub = X_train[idx]\n",
    "    y_train_sub = y_train[idx]\n",
    "    print(f\"Subsampled training size for XGBoost: {X_train_sub.shape[0]}\")\n",
    "else:\n",
    "    X_train_sub = X_train\n",
    "    y_train_sub = y_train\n",
    "    print(\"Using full training set for XGBoost.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633462e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================\n",
    "# 7. Evaluation Helper\n",
    "# ==============================================================\n",
    "\n",
    "def evaluate_classifier(name, clf, X_val, y_val, X_test, y_test):\n",
    "    attack_names = [attack_label_mapping[str(i)] for i in range(len(attack_label_mapping))]\n",
    "\n",
    "    for split_name, X, y in [(\"Val\", X_val, y_val), (\"Test\", X_test, y_test)]:\n",
    "        y_pred = clf.predict(X)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        macro_f1 = f1_score(y, y_pred, average=\"macro\")\n",
    "\n",
    "        print(f\"\\n[{name}] {split_name} Accuracy: {acc:.4f}, Macro-F1: {macro_f1:.4f}\")\n",
    "        print(f\"[{name}] {split_name} classification report:\")\n",
    "        print(classification_report(\n",
    "            y,\n",
    "            y_pred,\n",
    "            target_names=attack_names,\n",
    "            digits=4,\n",
    "            zero_division=0,\n",
    "        ))\n",
    "        print(f\"[{name}] {split_name} confusion matrix:\")\n",
    "        print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b71b033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost baseline with config: {'n_estimators': 400, 'max_depth': 8, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'multi:softmax', 'tree_method': 'hist', 'eval_metric': 'mlogloss', 'n_jobs': -1, 'random_state': 42, 'num_class': 8}\n",
      "XGBoost training complete.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 8. Train XGBoost Baseline\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\nTraining XGBoost baseline with config:\", XGB_CONFIG)\n",
    "xgb = XGBClassifier(**XGB_CONFIG)\n",
    "xgb.fit(X_train_sub, y_train_sub)\n",
    "print(\"XGBoost training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ce3e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[XGBoost] Val Accuracy: 0.9955, Macro-F1: 0.9945\n",
      "[XGBoost] Val classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9944    0.9967    0.9956     67500\n",
      " brute force     0.9983    0.9923    0.9953     19721\n",
      "        ddos     0.9989    0.9968    0.9978     67500\n",
      "         dos     0.9956    0.9968    0.9962     67500\n",
      "       mirai     0.9995    0.9970    0.9982     67500\n",
      "       recon     0.9973    0.9933    0.9953     67500\n",
      "    spoofing     0.9975    0.9951    0.9963     67500\n",
      "   web-based     0.9713    0.9915    0.9813     30911\n",
      "\n",
      "    accuracy                         0.9955    455632\n",
      "   macro avg     0.9941    0.9949    0.9945    455632\n",
      "weighted avg     0.9955    0.9955    0.9955    455632\n",
      "\n",
      "[XGBoost] Val confusion matrix:\n",
      "[[67275     9     6    27     2    11    21   149]\n",
      " [   47 19569     4    13     1     5     5    77]\n",
      " [   56     8 67281    92     3    13     3    44]\n",
      " [   62     6    33 67282     7    24    15    71]\n",
      " [   17     1     5    22 67297     5    48   105]\n",
      " [   67     1    15    85     1 67045    25   261]\n",
      " [   43     3     3    25    14    48 67167   197]\n",
      " [   84     5     7    30     8    77    52 30648]]\n",
      "\n",
      "[XGBoost] Test Accuracy: 0.9952, Macro-F1: 0.9942\n",
      "[XGBoost] Test classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9935    0.9968    0.9952     67500\n",
      " brute force     0.9985    0.9923    0.9954     19722\n",
      "        ddos     0.9988    0.9965    0.9977     67500\n",
      "         dos     0.9957    0.9963    0.9960     67500\n",
      "       mirai     0.9995    0.9968    0.9982     67500\n",
      "       recon     0.9969    0.9924    0.9947     67500\n",
      "    spoofing     0.9968    0.9950    0.9959     67500\n",
      "   web-based     0.9709    0.9905    0.9806     30910\n",
      "\n",
      "    accuracy                         0.9952    455632\n",
      "   macro avg     0.9938    0.9946    0.9942    455632\n",
      "weighted avg     0.9952    0.9952    0.9952    455632\n",
      "\n",
      "[XGBoost] Test confusion matrix:\n",
      "[[67285     7     2    34     4    10    26   132]\n",
      " [   56 19570     3     9     1    10     1    72]\n",
      " [   51    16 67267    91     7    16     5    47]\n",
      " [   85     0    42 67253     6    28    15    71]\n",
      " [   12     1     9    19 67283     8    67   101]\n",
      " [   81     3    15    83     2 66989    37   290]\n",
      " [   45     2     2    23    10    50 67164   204]\n",
      " [  109     1     5    30     2    83    63 30617]]\n",
      "\n",
      "Done (XGBoost baseline).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================\n",
    "# 9. Evaluate\n",
    "# ==============================================================\n",
    "\n",
    "evaluate_classifier(\"XGBoost\", xgb, X_val, y_val, X_test, y_test)\n",
    "\n",
    "print(\"\\nDone (XGBoost baseline).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
