{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88246696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS\n",
      "Using processed data from: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed\n",
      "Using device: mps\n",
      "\n",
      "Loading train/val/test CSVs (full)...\n",
      "Train shape: (2126280, 139)\n",
      "Val   shape: (455632, 139)\n",
      "Test  shape: (455632, 139)\n",
      "Number of attack classes: 8\n",
      "\n",
      "Number of feature columns: 120\n",
      "Example features: ['stream', 'src_port', 'dst_port', 'inter_arrival_time', 'time_since_previously_displayed_frame', 'port_class_dst', 'l4_tcp', 'l4_udp', 'ttl', 'eth_size', 'tcp_window_size', 'payload_entropy', 'handshake_cipher_suites_length', 'handshake_ciphersuites', 'handshake_extensions_length']\n",
      "\n",
      "Cleaning NaN/Inf...\n",
      "  [train] NaN before fill: 20652772, filling with 0.\n",
      "  [val] NaN before fill: 4432636, filling with 0.\n",
      "  [test] NaN before fill: 4426542, filling with 0.\n",
      "\n",
      "Standardising features...\n",
      "  [train] NaN after std: 0, Inf: 0\n",
      "  [val] NaN after std: 0, Inf: 0\n",
      "  [test] NaN after std: 0, Inf: 0\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 2126280\n",
      "  Val  : 455632\n",
      "  Test : 455632\n",
      "\n",
      "Building single-task MLP:\n",
      "  num_features: 120\n",
      "  num_attacks : 8\n",
      "Total parameters: 229000\n",
      "Dummy logits shape: torch.Size([8, 8])\n",
      "\n",
      "Scheduler: warmup_steps=6228, total_steps=124560\n",
      "\n",
      "============================================================\n",
      "Starting single-task training for 30 epochs\n",
      "============================================================\n",
      "\n",
      "Epoch 1/30\n",
      "Epoch 1 | Batch 200/4152 | Loss: 2.4921 | Batch Acc: 0.152 | LR: 9.63e-06\n",
      "Epoch 1 | Batch 400/4152 | Loss: 1.6297 | Batch Acc: 0.461 | LR: 1.93e-05\n",
      "Epoch 1 | Batch 600/4152 | Loss: 1.0997 | Batch Acc: 0.645 | LR: 2.89e-05\n",
      "Epoch 1 | Batch 800/4152 | Loss: 0.8675 | Batch Acc: 0.736 | LR: 3.85e-05\n",
      "Epoch 1 | Batch 1000/4152 | Loss: 0.7347 | Batch Acc: 0.746 | LR: 4.82e-05\n",
      "Epoch 1 | Batch 1200/4152 | Loss: 0.7352 | Batch Acc: 0.730 | LR: 5.78e-05\n",
      "Epoch 1 | Batch 1400/4152 | Loss: 0.6960 | Batch Acc: 0.744 | LR: 6.74e-05\n",
      "Epoch 1 | Batch 1600/4152 | Loss: 0.6488 | Batch Acc: 0.773 | LR: 7.71e-05\n",
      "Epoch 1 | Batch 1800/4152 | Loss: 0.4861 | Batch Acc: 0.846 | LR: 8.67e-05\n",
      "Epoch 1 | Batch 2000/4152 | Loss: 0.5356 | Batch Acc: 0.799 | LR: 9.63e-05\n",
      "Epoch 1 | Batch 2200/4152 | Loss: 0.4850 | Batch Acc: 0.844 | LR: 1.06e-04\n",
      "Epoch 1 | Batch 2400/4152 | Loss: 0.5311 | Batch Acc: 0.811 | LR: 1.16e-04\n",
      "Epoch 1 | Batch 2600/4152 | Loss: 0.5554 | Batch Acc: 0.801 | LR: 1.25e-04\n",
      "Epoch 1 | Batch 2800/4152 | Loss: 0.5100 | Batch Acc: 0.799 | LR: 1.35e-04\n",
      "Epoch 1 | Batch 3000/4152 | Loss: 0.4899 | Batch Acc: 0.826 | LR: 1.45e-04\n",
      "Epoch 1 | Batch 3200/4152 | Loss: 0.4523 | Batch Acc: 0.834 | LR: 1.54e-04\n",
      "Epoch 1 | Batch 3400/4152 | Loss: 0.4642 | Batch Acc: 0.834 | LR: 1.64e-04\n",
      "Epoch 1 | Batch 3600/4152 | Loss: 0.4245 | Batch Acc: 0.869 | LR: 1.73e-04\n",
      "Epoch 1 | Batch 3800/4152 | Loss: 0.4035 | Batch Acc: 0.848 | LR: 1.83e-04\n",
      "Epoch 1 | Batch 4000/4152 | Loss: 0.4175 | Batch Acc: 0.859 | LR: 1.93e-04\n",
      "  Train Loss: 0.7660, Train Acc: 0.7342\n",
      "  Val   Loss: 0.3358, Val   Acc: 0.8782\n",
      "  ✓ New best model saved (val_acc=0.8782)\n",
      "\n",
      "Epoch 2/30\n",
      "Epoch 2 | Batch 200/4152 | Loss: 0.4029 | Batch Acc: 0.854 | LR: 2.10e-04\n",
      "Epoch 2 | Batch 400/4152 | Loss: 0.4389 | Batch Acc: 0.838 | LR: 2.19e-04\n",
      "Epoch 2 | Batch 600/4152 | Loss: 0.3341 | Batch Acc: 0.881 | LR: 2.29e-04\n",
      "Epoch 2 | Batch 800/4152 | Loss: 0.3751 | Batch Acc: 0.873 | LR: 2.39e-04\n",
      "Epoch 2 | Batch 1000/4152 | Loss: 0.4532 | Batch Acc: 0.842 | LR: 2.48e-04\n",
      "Epoch 2 | Batch 1200/4152 | Loss: 0.3159 | Batch Acc: 0.881 | LR: 2.58e-04\n",
      "Epoch 2 | Batch 1400/4152 | Loss: 0.3785 | Batch Acc: 0.855 | LR: 2.67e-04\n",
      "Epoch 2 | Batch 1600/4152 | Loss: 0.2469 | Batch Acc: 0.912 | LR: 2.77e-04\n",
      "Epoch 2 | Batch 1800/4152 | Loss: 0.3161 | Batch Acc: 0.885 | LR: 2.87e-04\n",
      "Epoch 2 | Batch 2000/4152 | Loss: 0.3132 | Batch Acc: 0.898 | LR: 2.96e-04\n",
      "Epoch 2 | Batch 2200/4152 | Loss: 0.2674 | Batch Acc: 0.887 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 2400/4152 | Loss: 0.2934 | Batch Acc: 0.885 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 2600/4152 | Loss: 0.3516 | Batch Acc: 0.871 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 2800/4152 | Loss: 0.3003 | Batch Acc: 0.896 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 3000/4152 | Loss: 0.2785 | Batch Acc: 0.910 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 3200/4152 | Loss: 0.2372 | Batch Acc: 0.916 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 3400/4152 | Loss: 0.2354 | Batch Acc: 0.916 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 3600/4152 | Loss: 0.2316 | Batch Acc: 0.914 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 3800/4152 | Loss: 0.2919 | Batch Acc: 0.896 | LR: 3.00e-04\n",
      "Epoch 2 | Batch 4000/4152 | Loss: 0.2412 | Batch Acc: 0.912 | LR: 3.00e-04\n",
      "  Train Loss: 0.3198, Train Acc: 0.8854\n",
      "  Val   Loss: 0.2120, Val   Acc: 0.9230\n",
      "  ✓ New best model saved (val_acc=0.9230)\n",
      "\n",
      "Epoch 3/30\n",
      "Epoch 3 | Batch 200/4152 | Loss: 0.2382 | Batch Acc: 0.912 | LR: 3.00e-04\n",
      "Epoch 3 | Batch 400/4152 | Loss: 0.2563 | Batch Acc: 0.902 | LR: 3.00e-04\n",
      "Epoch 3 | Batch 600/4152 | Loss: 0.2164 | Batch Acc: 0.924 | LR: 3.00e-04\n",
      "Epoch 3 | Batch 800/4152 | Loss: 0.2820 | Batch Acc: 0.910 | LR: 3.00e-04\n",
      "Epoch 3 | Batch 1000/4152 | Loss: 0.2107 | Batch Acc: 0.926 | LR: 3.00e-04\n",
      "Epoch 3 | Batch 1200/4152 | Loss: 0.1874 | Batch Acc: 0.928 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 1400/4152 | Loss: 0.1936 | Batch Acc: 0.945 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 1600/4152 | Loss: 0.2574 | Batch Acc: 0.912 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 1800/4152 | Loss: 0.2518 | Batch Acc: 0.904 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 2000/4152 | Loss: 0.2584 | Batch Acc: 0.916 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 2200/4152 | Loss: 0.2044 | Batch Acc: 0.930 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 2400/4152 | Loss: 0.2414 | Batch Acc: 0.908 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 2600/4152 | Loss: 0.2048 | Batch Acc: 0.926 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 2800/4152 | Loss: 0.2268 | Batch Acc: 0.914 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 3000/4152 | Loss: 0.2513 | Batch Acc: 0.908 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 3200/4152 | Loss: 0.2342 | Batch Acc: 0.926 | LR: 2.99e-04\n",
      "Epoch 3 | Batch 3400/4152 | Loss: 0.2310 | Batch Acc: 0.930 | LR: 2.98e-04\n",
      "Epoch 3 | Batch 3600/4152 | Loss: 0.2738 | Batch Acc: 0.891 | LR: 2.98e-04\n",
      "Epoch 3 | Batch 3800/4152 | Loss: 0.2389 | Batch Acc: 0.908 | LR: 2.98e-04\n",
      "Epoch 3 | Batch 4000/4152 | Loss: 0.2164 | Batch Acc: 0.924 | LR: 2.98e-04\n",
      "  Train Loss: 0.2351, Train Acc: 0.9155\n",
      "  Val   Loss: 0.1719, Val   Acc: 0.9380\n",
      "  ✓ New best model saved (val_acc=0.9380)\n",
      "\n",
      "Epoch 4/30\n",
      "Epoch 4 | Batch 200/4152 | Loss: 0.2067 | Batch Acc: 0.928 | LR: 2.98e-04\n",
      "Epoch 4 | Batch 400/4152 | Loss: 0.2079 | Batch Acc: 0.934 | LR: 2.98e-04\n",
      "Epoch 4 | Batch 600/4152 | Loss: 0.1908 | Batch Acc: 0.934 | LR: 2.98e-04\n",
      "Epoch 4 | Batch 800/4152 | Loss: 0.2108 | Batch Acc: 0.928 | LR: 2.97e-04\n",
      "Epoch 4 | Batch 1000/4152 | Loss: 0.1715 | Batch Acc: 0.943 | LR: 2.97e-04\n",
      "Epoch 4 | Batch 1200/4152 | Loss: 0.1936 | Batch Acc: 0.932 | LR: 2.97e-04\n",
      "Epoch 4 | Batch 1400/4152 | Loss: 0.2385 | Batch Acc: 0.916 | LR: 2.97e-04\n",
      "Epoch 4 | Batch 1600/4152 | Loss: 0.1964 | Batch Acc: 0.924 | LR: 2.97e-04\n",
      "Epoch 4 | Batch 1800/4152 | Loss: 0.1698 | Batch Acc: 0.932 | LR: 2.97e-04\n",
      "Epoch 4 | Batch 2000/4152 | Loss: 0.2370 | Batch Acc: 0.922 | LR: 2.96e-04\n",
      "Epoch 4 | Batch 2200/4152 | Loss: 0.2118 | Batch Acc: 0.924 | LR: 2.96e-04\n",
      "Epoch 4 | Batch 2400/4152 | Loss: 0.1953 | Batch Acc: 0.924 | LR: 2.96e-04\n",
      "Epoch 4 | Batch 2600/4152 | Loss: 0.1885 | Batch Acc: 0.930 | LR: 2.96e-04\n",
      "Epoch 4 | Batch 2800/4152 | Loss: 0.2160 | Batch Acc: 0.920 | LR: 2.96e-04\n",
      "Epoch 4 | Batch 3000/4152 | Loss: 0.2173 | Batch Acc: 0.916 | LR: 2.96e-04\n",
      "Epoch 4 | Batch 3200/4152 | Loss: 0.2108 | Batch Acc: 0.918 | LR: 2.95e-04\n",
      "Epoch 4 | Batch 3400/4152 | Loss: 0.1479 | Batch Acc: 0.947 | LR: 2.95e-04\n",
      "Epoch 4 | Batch 3600/4152 | Loss: 0.1523 | Batch Acc: 0.939 | LR: 2.95e-04\n",
      "Epoch 4 | Batch 3800/4152 | Loss: 0.1760 | Batch Acc: 0.945 | LR: 2.95e-04\n",
      "Epoch 4 | Batch 4000/4152 | Loss: 0.1832 | Batch Acc: 0.930 | LR: 2.95e-04\n",
      "  Train Loss: 0.2022, Train Acc: 0.9267\n",
      "  Val   Loss: 0.1511, Val   Acc: 0.9448\n",
      "  ✓ New best model saved (val_acc=0.9448)\n",
      "\n",
      "Epoch 5/30\n",
      "Epoch 5 | Batch 200/4152 | Loss: 0.2034 | Batch Acc: 0.924 | LR: 2.94e-04\n",
      "Epoch 5 | Batch 400/4152 | Loss: 0.1651 | Batch Acc: 0.936 | LR: 2.94e-04\n",
      "Epoch 5 | Batch 600/4152 | Loss: 0.1845 | Batch Acc: 0.920 | LR: 2.94e-04\n",
      "Epoch 5 | Batch 800/4152 | Loss: 0.2334 | Batch Acc: 0.910 | LR: 2.93e-04\n",
      "Epoch 5 | Batch 1000/4152 | Loss: 0.1676 | Batch Acc: 0.934 | LR: 2.93e-04\n",
      "Epoch 5 | Batch 1200/4152 | Loss: 0.1485 | Batch Acc: 0.939 | LR: 2.93e-04\n",
      "Epoch 5 | Batch 1400/4152 | Loss: 0.2242 | Batch Acc: 0.914 | LR: 2.93e-04\n",
      "Epoch 5 | Batch 1600/4152 | Loss: 0.1771 | Batch Acc: 0.945 | LR: 2.92e-04\n",
      "Epoch 5 | Batch 1800/4152 | Loss: 0.2279 | Batch Acc: 0.908 | LR: 2.92e-04\n",
      "Epoch 5 | Batch 2000/4152 | Loss: 0.1536 | Batch Acc: 0.947 | LR: 2.92e-04\n",
      "Epoch 5 | Batch 2200/4152 | Loss: 0.1703 | Batch Acc: 0.932 | LR: 2.92e-04\n",
      "Epoch 5 | Batch 2400/4152 | Loss: 0.1855 | Batch Acc: 0.920 | LR: 2.91e-04\n",
      "Epoch 5 | Batch 2600/4152 | Loss: 0.1684 | Batch Acc: 0.934 | LR: 2.91e-04\n",
      "Epoch 5 | Batch 2800/4152 | Loss: 0.2254 | Batch Acc: 0.914 | LR: 2.91e-04\n",
      "Epoch 5 | Batch 3000/4152 | Loss: 0.1819 | Batch Acc: 0.932 | LR: 2.91e-04\n",
      "Epoch 5 | Batch 3200/4152 | Loss: 0.2130 | Batch Acc: 0.918 | LR: 2.90e-04\n",
      "Epoch 5 | Batch 3400/4152 | Loss: 0.1377 | Batch Acc: 0.953 | LR: 2.90e-04\n",
      "Epoch 5 | Batch 3600/4152 | Loss: 0.2213 | Batch Acc: 0.916 | LR: 2.90e-04\n",
      "Epoch 5 | Batch 3800/4152 | Loss: 0.1477 | Batch Acc: 0.938 | LR: 2.89e-04\n",
      "Epoch 5 | Batch 4000/4152 | Loss: 0.2235 | Batch Acc: 0.932 | LR: 2.89e-04\n",
      "  Train Loss: 0.1833, Train Acc: 0.9333\n",
      "  Val   Loss: 0.1406, Val   Acc: 0.9481\n",
      "  ✓ New best model saved (val_acc=0.9481)\n",
      "\n",
      "Epoch 6/30\n",
      "Epoch 6 | Batch 200/4152 | Loss: 0.1769 | Batch Acc: 0.934 | LR: 2.89e-04\n",
      "Epoch 6 | Batch 400/4152 | Loss: 0.1647 | Batch Acc: 0.943 | LR: 2.88e-04\n",
      "Epoch 6 | Batch 600/4152 | Loss: 0.1956 | Batch Acc: 0.922 | LR: 2.88e-04\n",
      "Epoch 6 | Batch 800/4152 | Loss: 0.2125 | Batch Acc: 0.916 | LR: 2.88e-04\n",
      "Epoch 6 | Batch 1000/4152 | Loss: 0.1726 | Batch Acc: 0.953 | LR: 2.87e-04\n",
      "Epoch 6 | Batch 1200/4152 | Loss: 0.2017 | Batch Acc: 0.924 | LR: 2.87e-04\n",
      "Epoch 6 | Batch 1400/4152 | Loss: 0.2069 | Batch Acc: 0.932 | LR: 2.87e-04\n",
      "Epoch 6 | Batch 1600/4152 | Loss: 0.2092 | Batch Acc: 0.920 | LR: 2.86e-04\n",
      "Epoch 6 | Batch 1800/4152 | Loss: 0.1427 | Batch Acc: 0.947 | LR: 2.86e-04\n",
      "Epoch 6 | Batch 2000/4152 | Loss: 0.2349 | Batch Acc: 0.910 | LR: 2.86e-04\n",
      "Epoch 6 | Batch 2200/4152 | Loss: 0.1534 | Batch Acc: 0.936 | LR: 2.85e-04\n",
      "Epoch 6 | Batch 2400/4152 | Loss: 0.2265 | Batch Acc: 0.928 | LR: 2.85e-04\n",
      "Epoch 6 | Batch 2600/4152 | Loss: 0.1377 | Batch Acc: 0.953 | LR: 2.85e-04\n",
      "Epoch 6 | Batch 2800/4152 | Loss: 0.1419 | Batch Acc: 0.953 | LR: 2.84e-04\n",
      "Epoch 6 | Batch 3000/4152 | Loss: 0.1278 | Batch Acc: 0.951 | LR: 2.84e-04\n",
      "Epoch 6 | Batch 3200/4152 | Loss: 0.1880 | Batch Acc: 0.926 | LR: 2.84e-04\n",
      "Epoch 6 | Batch 3400/4152 | Loss: 0.1810 | Batch Acc: 0.939 | LR: 2.83e-04\n",
      "Epoch 6 | Batch 3600/4152 | Loss: 0.1677 | Batch Acc: 0.932 | LR: 2.83e-04\n",
      "Epoch 6 | Batch 3800/4152 | Loss: 0.1657 | Batch Acc: 0.943 | LR: 2.83e-04\n",
      "Epoch 6 | Batch 4000/4152 | Loss: 0.1944 | Batch Acc: 0.922 | LR: 2.82e-04\n",
      "  Train Loss: 0.1706, Train Acc: 0.9378\n",
      "  Val   Loss: 0.1307, Val   Acc: 0.9524\n",
      "  ✓ New best model saved (val_acc=0.9524)\n",
      "\n",
      "Epoch 7/30\n",
      "Epoch 7 | Batch 200/4152 | Loss: 0.1613 | Batch Acc: 0.947 | LR: 2.82e-04\n",
      "Epoch 7 | Batch 400/4152 | Loss: 0.1474 | Batch Acc: 0.945 | LR: 2.81e-04\n",
      "Epoch 7 | Batch 600/4152 | Loss: 0.1595 | Batch Acc: 0.945 | LR: 2.81e-04\n",
      "Epoch 7 | Batch 800/4152 | Loss: 0.1997 | Batch Acc: 0.924 | LR: 2.80e-04\n",
      "Epoch 7 | Batch 1000/4152 | Loss: 0.1529 | Batch Acc: 0.945 | LR: 2.80e-04\n",
      "Epoch 7 | Batch 1200/4152 | Loss: 0.1700 | Batch Acc: 0.932 | LR: 2.80e-04\n",
      "Epoch 7 | Batch 1400/4152 | Loss: 0.2230 | Batch Acc: 0.914 | LR: 2.79e-04\n",
      "Epoch 7 | Batch 1600/4152 | Loss: 0.1781 | Batch Acc: 0.932 | LR: 2.79e-04\n",
      "Epoch 7 | Batch 1800/4152 | Loss: 0.1312 | Batch Acc: 0.955 | LR: 2.78e-04\n",
      "Epoch 7 | Batch 2000/4152 | Loss: 0.1949 | Batch Acc: 0.943 | LR: 2.78e-04\n",
      "Epoch 7 | Batch 2200/4152 | Loss: 0.1884 | Batch Acc: 0.932 | LR: 2.78e-04\n",
      "Epoch 7 | Batch 2400/4152 | Loss: 0.1170 | Batch Acc: 0.965 | LR: 2.77e-04\n",
      "Epoch 7 | Batch 2600/4152 | Loss: 0.1679 | Batch Acc: 0.938 | LR: 2.77e-04\n",
      "Epoch 7 | Batch 2800/4152 | Loss: 0.2303 | Batch Acc: 0.920 | LR: 2.76e-04\n",
      "Epoch 7 | Batch 3000/4152 | Loss: 0.1461 | Batch Acc: 0.953 | LR: 2.76e-04\n",
      "Epoch 7 | Batch 3200/4152 | Loss: 0.1161 | Batch Acc: 0.953 | LR: 2.75e-04\n",
      "Epoch 7 | Batch 3400/4152 | Loss: 0.1148 | Batch Acc: 0.967 | LR: 2.75e-04\n",
      "Epoch 7 | Batch 3600/4152 | Loss: 0.1869 | Batch Acc: 0.938 | LR: 2.75e-04\n",
      "Epoch 7 | Batch 3800/4152 | Loss: 0.1818 | Batch Acc: 0.934 | LR: 2.74e-04\n",
      "Epoch 7 | Batch 4000/4152 | Loss: 0.1545 | Batch Acc: 0.943 | LR: 2.74e-04\n",
      "  Train Loss: 0.1618, Train Acc: 0.9411\n",
      "  Val   Loss: 0.1235, Val   Acc: 0.9543\n",
      "  ✓ New best model saved (val_acc=0.9543)\n",
      "\n",
      "Epoch 8/30\n",
      "Epoch 8 | Batch 200/4152 | Loss: 0.1381 | Batch Acc: 0.953 | LR: 2.73e-04\n",
      "Epoch 8 | Batch 400/4152 | Loss: 0.1938 | Batch Acc: 0.928 | LR: 2.72e-04\n",
      "Epoch 8 | Batch 600/4152 | Loss: 0.1955 | Batch Acc: 0.936 | LR: 2.72e-04\n",
      "Epoch 8 | Batch 800/4152 | Loss: 0.1711 | Batch Acc: 0.934 | LR: 2.71e-04\n",
      "Epoch 8 | Batch 1000/4152 | Loss: 0.1620 | Batch Acc: 0.949 | LR: 2.71e-04\n",
      "Epoch 8 | Batch 1200/4152 | Loss: 0.1414 | Batch Acc: 0.957 | LR: 2.70e-04\n",
      "Epoch 8 | Batch 1400/4152 | Loss: 0.1570 | Batch Acc: 0.949 | LR: 2.70e-04\n",
      "Epoch 8 | Batch 1600/4152 | Loss: 0.1666 | Batch Acc: 0.943 | LR: 2.70e-04\n",
      "Epoch 8 | Batch 1800/4152 | Loss: 0.1193 | Batch Acc: 0.955 | LR: 2.69e-04\n",
      "Epoch 8 | Batch 2000/4152 | Loss: 0.1704 | Batch Acc: 0.941 | LR: 2.69e-04\n",
      "Epoch 8 | Batch 2200/4152 | Loss: 0.1489 | Batch Acc: 0.947 | LR: 2.68e-04\n",
      "Epoch 8 | Batch 2400/4152 | Loss: 0.1341 | Batch Acc: 0.949 | LR: 2.68e-04\n",
      "Epoch 8 | Batch 2600/4152 | Loss: 0.1813 | Batch Acc: 0.932 | LR: 2.67e-04\n",
      "Epoch 8 | Batch 2800/4152 | Loss: 0.1769 | Batch Acc: 0.926 | LR: 2.67e-04\n",
      "Epoch 8 | Batch 3000/4152 | Loss: 0.1844 | Batch Acc: 0.930 | LR: 2.66e-04\n",
      "Epoch 8 | Batch 3200/4152 | Loss: 0.1934 | Batch Acc: 0.934 | LR: 2.66e-04\n",
      "Epoch 8 | Batch 3400/4152 | Loss: 0.1542 | Batch Acc: 0.943 | LR: 2.65e-04\n",
      "Epoch 8 | Batch 3600/4152 | Loss: 0.1153 | Batch Acc: 0.955 | LR: 2.65e-04\n",
      "Epoch 8 | Batch 3800/4152 | Loss: 0.1823 | Batch Acc: 0.924 | LR: 2.64e-04\n",
      "Epoch 8 | Batch 4000/4152 | Loss: 0.1624 | Batch Acc: 0.943 | LR: 2.64e-04\n",
      "  Train Loss: 0.1539, Train Acc: 0.9439\n",
      "  Val   Loss: 0.1181, Val   Acc: 0.9564\n",
      "  ✓ New best model saved (val_acc=0.9564)\n",
      "\n",
      "Epoch 9/30\n",
      "Epoch 9 | Batch 200/4152 | Loss: 0.1260 | Batch Acc: 0.951 | LR: 2.63e-04\n",
      "Epoch 9 | Batch 400/4152 | Loss: 0.1493 | Batch Acc: 0.941 | LR: 2.62e-04\n",
      "Epoch 9 | Batch 600/4152 | Loss: 0.1509 | Batch Acc: 0.939 | LR: 2.62e-04\n",
      "Epoch 9 | Batch 800/4152 | Loss: 0.1645 | Batch Acc: 0.947 | LR: 2.61e-04\n",
      "Epoch 9 | Batch 1000/4152 | Loss: 0.1802 | Batch Acc: 0.930 | LR: 2.60e-04\n",
      "Epoch 9 | Batch 1200/4152 | Loss: 0.1759 | Batch Acc: 0.943 | LR: 2.60e-04\n",
      "Epoch 9 | Batch 1400/4152 | Loss: 0.1139 | Batch Acc: 0.961 | LR: 2.59e-04\n",
      "Epoch 9 | Batch 1600/4152 | Loss: 0.1185 | Batch Acc: 0.955 | LR: 2.59e-04\n",
      "Epoch 9 | Batch 1800/4152 | Loss: 0.1433 | Batch Acc: 0.943 | LR: 2.58e-04\n",
      "Epoch 9 | Batch 2000/4152 | Loss: 0.1178 | Batch Acc: 0.965 | LR: 2.58e-04\n",
      "Epoch 9 | Batch 2200/4152 | Loss: 0.1669 | Batch Acc: 0.941 | LR: 2.57e-04\n",
      "Epoch 9 | Batch 2400/4152 | Loss: 0.1309 | Batch Acc: 0.957 | LR: 2.57e-04\n",
      "Epoch 9 | Batch 2600/4152 | Loss: 0.1714 | Batch Acc: 0.945 | LR: 2.56e-04\n",
      "Epoch 9 | Batch 2800/4152 | Loss: 0.1764 | Batch Acc: 0.941 | LR: 2.55e-04\n",
      "Epoch 9 | Batch 3000/4152 | Loss: 0.1422 | Batch Acc: 0.957 | LR: 2.55e-04\n",
      "Epoch 9 | Batch 3200/4152 | Loss: 0.1882 | Batch Acc: 0.930 | LR: 2.54e-04\n",
      "Epoch 9 | Batch 3400/4152 | Loss: 0.1884 | Batch Acc: 0.936 | LR: 2.54e-04\n",
      "Epoch 9 | Batch 3600/4152 | Loss: 0.1949 | Batch Acc: 0.936 | LR: 2.53e-04\n",
      "Epoch 9 | Batch 3800/4152 | Loss: 0.1899 | Batch Acc: 0.922 | LR: 2.53e-04\n",
      "Epoch 9 | Batch 4000/4152 | Loss: 0.1608 | Batch Acc: 0.943 | LR: 2.52e-04\n",
      "  Train Loss: 0.1482, Train Acc: 0.9460\n",
      "  Val   Loss: 0.1136, Val   Acc: 0.9585\n",
      "  ✓ New best model saved (val_acc=0.9585)\n",
      "\n",
      "Epoch 10/30\n",
      "Epoch 10 | Batch 200/4152 | Loss: 0.1791 | Batch Acc: 0.939 | LR: 2.51e-04\n",
      "Epoch 10 | Batch 400/4152 | Loss: 0.1238 | Batch Acc: 0.957 | LR: 2.50e-04\n",
      "Epoch 10 | Batch 600/4152 | Loss: 0.1380 | Batch Acc: 0.947 | LR: 2.50e-04\n",
      "Epoch 10 | Batch 800/4152 | Loss: 0.1289 | Batch Acc: 0.963 | LR: 2.49e-04\n",
      "Epoch 10 | Batch 1000/4152 | Loss: 0.1305 | Batch Acc: 0.961 | LR: 2.49e-04\n",
      "Epoch 10 | Batch 1200/4152 | Loss: 0.1336 | Batch Acc: 0.955 | LR: 2.48e-04\n",
      "Epoch 10 | Batch 1400/4152 | Loss: 0.1165 | Batch Acc: 0.957 | LR: 2.47e-04\n",
      "Epoch 10 | Batch 1600/4152 | Loss: 0.1220 | Batch Acc: 0.951 | LR: 2.47e-04\n",
      "Epoch 10 | Batch 1800/4152 | Loss: 0.1011 | Batch Acc: 0.971 | LR: 2.46e-04\n",
      "Epoch 10 | Batch 2000/4152 | Loss: 0.1753 | Batch Acc: 0.926 | LR: 2.46e-04\n",
      "Epoch 10 | Batch 2200/4152 | Loss: 0.1462 | Batch Acc: 0.945 | LR: 2.45e-04\n",
      "Epoch 10 | Batch 2400/4152 | Loss: 0.1359 | Batch Acc: 0.941 | LR: 2.44e-04\n",
      "Epoch 10 | Batch 2600/4152 | Loss: 0.1655 | Batch Acc: 0.943 | LR: 2.44e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 348\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     val_loss, val_acc, y_val_true, y_val_pred = evaluate(model, val_loader)\n\u001b[32m    351\u001b[39m     history.append({\n\u001b[32m    352\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: epoch,\n\u001b[32m    353\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m: train_loss,\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: scheduler.get_last_lr()[\u001b[32m0\u001b[39m],\n\u001b[32m    358\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scheduler, epoch)\u001b[39m\n\u001b[32m    271\u001b[39m logits = model(x)\n\u001b[32m    272\u001b[39m loss = criterion(logits, y)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n\u001b[32m    276\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Notebook: 08_single_task_attack_mlp.ipynb ---\n",
    "# Goal:\n",
    "# - Single-task Intrusion Detection (attack_id) on CIC IoT-IDAD 2024 packet-based CSVs\n",
    "# - Ignore device_id for now\n",
    "# - Use robust preprocessing, simple but strong MLP baseline\n",
    "# - Evaluate if we can approach reported high attack accuracy\n",
    "\n",
    "# ============================================================\n",
    "# 0. Environment & paths\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[0]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Using processed data from:\", PROCESSED_DIR)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# 1. Config\n",
    "# ============================================================\n",
    "\n",
    "TRAIN_PATH = PROCESSED_DIR / \"packets_train.csv\"\n",
    "VAL_PATH   = PROCESSED_DIR / \"packets_val.csv\"\n",
    "TEST_PATH  = PROCESSED_DIR / \"packets_test.csv\"\n",
    "\n",
    "ATTACK_LABEL_MAP_PATH = PROCESSED_DIR / \"attack_label_mapping.json\"\n",
    "\n",
    "BATCH_SIZE    = 512\n",
    "NUM_EPOCHS    = 30   # enough to see convergence\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "MAX_GRAD_NORM = 1.0\n",
    "PATIENCE      = 5    # early stopping\n",
    "\n",
    "LABEL_SMOOTHING = 0.0  # start with plain CE for reproduction\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load data\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading train/val/test CSVs (full)...\")\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df   = pd.read_csv(VAL_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Val   shape:\", val_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "\n",
    "with open(ATTACK_LABEL_MAP_PATH, \"r\") as f:\n",
    "    attack_label_mapping = json.load(f)[\"id_to_attack\"]\n",
    "\n",
    "num_attacks = len(attack_label_mapping)\n",
    "print(\"Number of attack classes:\", num_attacks)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Single-task target + features\n",
    "# ============================================================\n",
    "\n",
    "TARGET_COL = \"attack_id\"\n",
    "\n",
    "numeric_cols = train_df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c != TARGET_COL]\n",
    "\n",
    "print(\"\\nNumber of feature columns:\", len(feature_cols))\n",
    "print(\"Example features:\", feature_cols[:15])\n",
    "\n",
    "# Sanity: ensure target present\n",
    "if TARGET_COL not in train_df.columns:\n",
    "    raise ValueError(f\"{TARGET_COL} not found in train_df.\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. Robust preprocessing (no NaN/Inf)\n",
    "# ============================================================\n",
    "\n",
    "def clean_df(df, feature_cols, name):\n",
    "    # Replace inf with NaN, then fillna with 0\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    nan_before = df[feature_cols].isna().sum().sum()\n",
    "    if nan_before > 0:\n",
    "        print(f\"  [{name}] NaN before fill: {nan_before}, filling with 0.\")\n",
    "        df[feature_cols] = df[feature_cols].fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"\\nCleaning NaN/Inf...\")\n",
    "train_df = clean_df(train_df, feature_cols, \"train\")\n",
    "val_df   = clean_df(val_df, feature_cols, \"val\")\n",
    "test_df  = clean_df(test_df, feature_cols, \"test\")\n",
    "\n",
    "# Standardise on train stats\n",
    "print(\"\\nStandardising features...\")\n",
    "means = train_df[feature_cols].mean()\n",
    "stds  = train_df[feature_cols].std().replace(0, 1.0)\n",
    "\n",
    "for df, name in [(train_df, \"train\"), (val_df, \"val\"), (test_df, \"test\")]:\n",
    "    df[feature_cols] = (df[feature_cols] - means) / stds\n",
    "    # Final clip to avoid extreme values\n",
    "    df[feature_cols] = df[feature_cols].clip(-10, 10)\n",
    "    n_nan = df[feature_cols].isna().sum().sum()\n",
    "    n_inf = np.isinf(df[feature_cols].values).sum()\n",
    "    print(f\"  [{name}] NaN after std: {n_nan}, Inf: {n_inf}\")\n",
    "    if n_nan > 0 or n_inf > 0:\n",
    "        raise ValueError(f\"Found NaN/Inf in {name} after standardisation.\")\n",
    "\n",
    "# Optionally save scaler\n",
    "scaler_path = PROCESSED_DIR / \"single_task_attack_scaler.json\"\n",
    "with open(scaler_path, \"w\") as f:\n",
    "    json.dump({\"means\": means.to_dict(), \"stds\": stds.to_dict()}, f, indent=2)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Dataset / DataLoader\n",
    "# ============================================================\n",
    "\n",
    "class AttackDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols, target_col=\"attack_id\"):\n",
    "        self.X = df[feature_cols].values.astype(np.float32)\n",
    "        self.y = df[target_col].values.astype(np.int64)\n",
    "        assert not np.isnan(self.X).any()\n",
    "        assert not np.isinf(self.X).any()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = AttackDataset(train_df, feature_cols)\n",
    "val_dataset   = AttackDataset(val_df, feature_cols)\n",
    "test_dataset  = AttackDataset(test_df, feature_cols)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          drop_last=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          drop_last=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          drop_last=False, num_workers=0)\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(\"  Train:\", len(train_dataset))\n",
    "print(\"  Val  :\", len(val_dataset))\n",
    "print(\"  Test :\", len(test_dataset))\n",
    "\n",
    "# ============================================================\n",
    "# 6. Single-task MLP model\n",
    "# ============================================================\n",
    "\n",
    "class AttackMLP(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_dims=(512, 256, 128), dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = num_features\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            in_dim = h\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.head(x)\n",
    "\n",
    "num_features = len(feature_cols)\n",
    "print(\"\\nBuilding single-task MLP:\")\n",
    "print(\"  num_features:\", num_features)\n",
    "print(\"  num_attacks :\", num_attacks)\n",
    "\n",
    "model = AttackMLP(\n",
    "    num_features=num_features,\n",
    "    num_classes=num_attacks,\n",
    "    hidden_dims=(512, 256, 128),\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "print(\"Total parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# quick sanity\n",
    "x_dummy = torch.randn(8, num_features, device=device)\n",
    "with torch.no_grad():\n",
    "    logits_dummy = model(x_dummy)\n",
    "    print(\"Dummy logits shape:\", logits_dummy.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 7. Loss, optimizer, scheduler\n",
    "# ============================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # no label smoothing for baseline\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(0.05 * total_steps)  # 5% warmup\n",
    "\n",
    "def get_warmup_cosine_schedule(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_warmup_cosine_schedule(optimizer, warmup_steps, total_steps)\n",
    "print(f\"\\nScheduler: warmup_steps={warmup_steps}, total_steps={total_steps}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. Training / evaluation\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            batch_acc = (preds == y).float().mean().item()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Epoch {epoch} | Batch {batch_idx+1}/{len(loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.3f} | LR: {lr:.2e}\")\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_y = []\n",
    "    all_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "            all_y.append(y.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    y_true = np.concatenate(all_y)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "\n",
    "    return avg_loss, avg_acc, y_true, y_pred\n",
    "\n",
    "# ============================================================\n",
    "# 9. Training loop with early stopping\n",
    "# ============================================================\n",
    "\n",
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "history = []\n",
    "\n",
    "best_model_dir = PROJECT_ROOT / \"models\"\n",
    "best_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_model_file = best_model_dir / \"single_task_attack_mlp_best.pt\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Starting single-task training for {NUM_EPOCHS} epochs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scheduler, epoch)\n",
    "    val_loss, val_acc, y_val_true, y_val_pred = evaluate(model, val_loader)\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"lr\": scheduler.get_last_lr()[0],\n",
    "    })\n",
    "\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc + 1e-4:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_file)\n",
    "        print(f\"  ✓ New best model saved (val_acc={val_acc:.4f})\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(\"\\nEarly stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Save history\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_df.to_csv(REPORTS_DIR / \"single_task_attack_mlp_history.csv\", index=False)\n",
    "\n",
    "# ============================================================\n",
    "# 10. Final test evaluation\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating best single-task model on TEST set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = AttackMLP(\n",
    "    num_features=num_features,\n",
    "    num_classes=num_attacks,\n",
    "    hidden_dims=(512, 256, 128),\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_file, map_location=device))\n",
    "\n",
    "test_loss, test_acc, y_test_true, y_test_pred = evaluate(best_model, test_loader)\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (attack_id):\")\n",
    "attack_names = [attack_label_mapping[str(i)] for i in range(num_attacks)]\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test_true,\n",
    "        y_test_pred,\n",
    "        target_names=attack_names,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nConfusion matrix (attack_id):\")\n",
    "print(confusion_matrix(y_test_true, y_test_pred))\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb278c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating best single-task model on TEST set\n",
      "============================================================\n",
      "\n",
      "Test Loss: 0.1145\n",
      "Test Accuracy: 0.9583\n",
      "\n",
      "Classification report (attack_id):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9009    0.9728    0.9354     67500\n",
      " brute force     0.9982    0.9630    0.9803     19722\n",
      "        ddos     0.9963    0.9863    0.9913     67500\n",
      "         dos     0.9880    0.9604    0.9740     67500\n",
      "       mirai     0.9991    0.9863    0.9926     67500\n",
      "       recon     0.9653    0.9255    0.9450     67500\n",
      "    spoofing     0.9819    0.9495    0.9654     67500\n",
      "   web-based     0.7922    0.8881    0.8374     30910\n",
      "\n",
      "    accuracy                         0.9583    455632\n",
      "   macro avg     0.9527    0.9540    0.9527    455632\n",
      "weighted avg     0.9608    0.9583    0.9590    455632\n",
      "\n",
      "\n",
      "Confusion matrix (attack_id):\n",
      "[[65661     2     1    53     6   326   125  1326]\n",
      " [  289 18993     1    36     2    27    65   309]\n",
      " [  239     3 66576   145     0   311     2   224]\n",
      " [ 1290     6   141 64827     9   626    53   548]\n",
      " [  243     1     3    39 66573    72   163   406]\n",
      " [ 1665     2    72   426     4 62471   478  2382]\n",
      " [  888     6    17    62    16   411 64094  2006]\n",
      " [ 2609    15    15    27    24   472   298 27450]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating best single-task model on TEST set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = AttackMLP(\n",
    "    num_features=num_features,\n",
    "    num_classes=num_attacks,\n",
    "    hidden_dims=(512, 256, 128),\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_file, map_location=device))\n",
    "\n",
    "test_loss, test_acc, y_test_true, y_test_pred = evaluate(best_model, test_loader)\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (attack_id):\")\n",
    "attack_names = [attack_label_mapping[str(i)] for i in range(num_attacks)]\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test_true,\n",
    "        y_test_pred,\n",
    "        target_names=attack_names,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nConfusion matrix (attack_id):\")\n",
    "print(confusion_matrix(y_test_true, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
