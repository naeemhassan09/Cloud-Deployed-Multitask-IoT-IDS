{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a64c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS\n",
      "Using processed data from: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed\n",
      "Using device: mps\n",
      "\n",
      "Loading processed CSVs...\n",
      "Train shape: (2126280, 139)\n",
      "Val   shape: (455632, 139)\n",
      "Test  shape: (455632, 139)\n",
      "Number of attack classes: 8\n",
      "Number of device classes: 94\n",
      "\n",
      "Number of feature columns: 119\n",
      "Example features: ['stream', 'src_port', 'dst_port', 'inter_arrival_time', 'time_since_previously_displayed_frame', 'port_class_dst', 'l4_tcp', 'l4_udp', 'ttl', 'eth_size', 'tcp_window_size', 'payload_entropy', 'handshake_cipher_suites_length', 'handshake_ciphersuites', 'handshake_extensions_length']\n",
      "\n",
      "Cleaning NaN/Inf...\n",
      "  [train] NaN before fill: 20652772, filling with 0.\n",
      "  [val] NaN before fill: 4432636, filling with 0.\n",
      "  [test] NaN before fill: 4426542, filling with 0.\n",
      "\n",
      "Standardising features...\n",
      "  [train] NaN after std: 0, Inf: 0\n",
      "  [val] NaN after std: 0, Inf: 0\n",
      "  [test] NaN after std: 0, Inf: 0\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 2126280\n",
      "  Val  : 455632\n",
      "  Test : 455632\n",
      "\n",
      "Building MultiTask 1D-CNN model:\n",
      "  num_features: 119\n",
      "  num_attacks : 8\n",
      "  num_devices : 94\n",
      "Total parameters: 121318\n",
      "Dummy attack logits shape: torch.Size([8, 8])\n",
      "Dummy device logits shape: torch.Size([8, 94])\n",
      "\n",
      "============================================================\n",
      "STAGE 1: Attack-only training (single-task)\n",
      "============================================================\n",
      "\n",
      "Epoch 1/6\n",
      "[Stage1] Epoch 1 | Batch 200/4152 | Loss: 1.2314 | Batch Acc: 0.666 | LR: 4.82e-05\n",
      "[Stage1] Epoch 1 | Batch 400/4152 | Loss: 0.8954 | Batch Acc: 0.734 | LR: 9.64e-05\n",
      "[Stage1] Epoch 1 | Batch 600/4152 | Loss: 0.7860 | Batch Acc: 0.805 | LR: 1.45e-04\n",
      "[Stage1] Epoch 1 | Batch 800/4152 | Loss: 0.7295 | Batch Acc: 0.828 | LR: 1.93e-04\n",
      "[Stage1] Epoch 1 | Batch 1000/4152 | Loss: 0.7476 | Batch Acc: 0.832 | LR: 2.41e-04\n",
      "[Stage1] Epoch 1 | Batch 1200/4152 | Loss: 0.6656 | Batch Acc: 0.836 | LR: 2.89e-04\n",
      "[Stage1] Epoch 1 | Batch 1400/4152 | Loss: 0.6542 | Batch Acc: 0.848 | LR: 3.00e-04\n",
      "[Stage1] Epoch 1 | Batch 1600/4152 | Loss: 0.6411 | Batch Acc: 0.863 | LR: 3.00e-04\n",
      "[Stage1] Epoch 1 | Batch 1800/4152 | Loss: 0.6266 | Batch Acc: 0.855 | LR: 3.00e-04\n",
      "[Stage1] Epoch 1 | Batch 2000/4152 | Loss: 0.6154 | Batch Acc: 0.865 | LR: 2.99e-04\n",
      "[Stage1] Epoch 1 | Batch 2200/4152 | Loss: 0.6240 | Batch Acc: 0.863 | LR: 2.99e-04\n",
      "[Stage1] Epoch 1 | Batch 2400/4152 | Loss: 0.6535 | Batch Acc: 0.852 | LR: 2.98e-04\n",
      "[Stage1] Epoch 1 | Batch 2600/4152 | Loss: 0.5900 | Batch Acc: 0.879 | LR: 2.98e-04\n",
      "[Stage1] Epoch 1 | Batch 2800/4152 | Loss: 0.6051 | Batch Acc: 0.875 | LR: 2.97e-04\n",
      "[Stage1] Epoch 1 | Batch 3000/4152 | Loss: 0.5563 | Batch Acc: 0.893 | LR: 2.96e-04\n",
      "[Stage1] Epoch 1 | Batch 3200/4152 | Loss: 0.5442 | Batch Acc: 0.904 | LR: 2.95e-04\n",
      "[Stage1] Epoch 1 | Batch 3400/4152 | Loss: 0.5513 | Batch Acc: 0.900 | LR: 2.94e-04\n",
      "[Stage1] Epoch 1 | Batch 3600/4152 | Loss: 0.5798 | Batch Acc: 0.879 | LR: 2.93e-04\n",
      "[Stage1] Epoch 1 | Batch 3800/4152 | Loss: 0.5552 | Batch Acc: 0.895 | LR: 2.91e-04\n",
      "[Stage1] Epoch 1 | Batch 4000/4152 | Loss: 0.5457 | Batch Acc: 0.895 | LR: 2.90e-04\n",
      "  [Stage1] Train Loss: 0.7129, Train Acc: 0.8306\n",
      "  [Stage1] Val   Loss: 0.5832, Val   Acc: 0.8760\n",
      "  ✓ [Stage1] New best attack-only model saved (val_acc=0.8760)\n",
      "\n",
      "Epoch 2/6\n",
      "[Stage1] Epoch 2 | Batch 200/4152 | Loss: 0.5421 | Batch Acc: 0.887 | LR: 2.87e-04\n",
      "[Stage1] Epoch 2 | Batch 400/4152 | Loss: 0.5003 | Batch Acc: 0.904 | LR: 2.86e-04\n",
      "[Stage1] Epoch 2 | Batch 600/4152 | Loss: 0.5270 | Batch Acc: 0.902 | LR: 2.84e-04\n",
      "[Stage1] Epoch 2 | Batch 800/4152 | Loss: 0.4934 | Batch Acc: 0.914 | LR: 2.82e-04\n",
      "[Stage1] Epoch 2 | Batch 1000/4152 | Loss: 0.5872 | Batch Acc: 0.887 | LR: 2.80e-04\n",
      "[Stage1] Epoch 2 | Batch 1200/4152 | Loss: 0.5427 | Batch Acc: 0.895 | LR: 2.78e-04\n",
      "[Stage1] Epoch 2 | Batch 1400/4152 | Loss: 0.5015 | Batch Acc: 0.924 | LR: 2.76e-04\n",
      "[Stage1] Epoch 2 | Batch 1600/4152 | Loss: 0.5292 | Batch Acc: 0.914 | LR: 2.74e-04\n",
      "[Stage1] Epoch 2 | Batch 1800/4152 | Loss: 0.5107 | Batch Acc: 0.906 | LR: 2.72e-04\n",
      "[Stage1] Epoch 2 | Batch 2000/4152 | Loss: 0.4857 | Batch Acc: 0.924 | LR: 2.69e-04\n",
      "[Stage1] Epoch 2 | Batch 2200/4152 | Loss: 0.4941 | Batch Acc: 0.922 | LR: 2.67e-04\n",
      "[Stage1] Epoch 2 | Batch 2400/4152 | Loss: 0.4922 | Batch Acc: 0.928 | LR: 2.64e-04\n",
      "[Stage1] Epoch 2 | Batch 2600/4152 | Loss: 0.5143 | Batch Acc: 0.914 | LR: 2.62e-04\n",
      "[Stage1] Epoch 2 | Batch 2800/4152 | Loss: 0.5152 | Batch Acc: 0.904 | LR: 2.59e-04\n",
      "[Stage1] Epoch 2 | Batch 3000/4152 | Loss: 0.4673 | Batch Acc: 0.951 | LR: 2.56e-04\n",
      "[Stage1] Epoch 2 | Batch 3200/4152 | Loss: 0.4813 | Batch Acc: 0.916 | LR: 2.53e-04\n",
      "[Stage1] Epoch 2 | Batch 3400/4152 | Loss: 0.4955 | Batch Acc: 0.928 | LR: 2.50e-04\n",
      "[Stage1] Epoch 2 | Batch 3600/4152 | Loss: 0.5613 | Batch Acc: 0.891 | LR: 2.47e-04\n",
      "[Stage1] Epoch 2 | Batch 3800/4152 | Loss: 0.4694 | Batch Acc: 0.930 | LR: 2.44e-04\n",
      "[Stage1] Epoch 2 | Batch 4000/4152 | Loss: 0.5201 | Batch Acc: 0.904 | LR: 2.41e-04\n",
      "  [Stage1] Train Loss: 0.5199, Train Acc: 0.9080\n",
      "  [Stage1] Val   Loss: 0.4879, Val   Acc: 0.9130\n",
      "  ✓ [Stage1] New best attack-only model saved (val_acc=0.9130)\n",
      "\n",
      "Epoch 3/6\n",
      "[Stage1] Epoch 3 | Batch 200/4152 | Loss: 0.4646 | Batch Acc: 0.938 | LR: 2.36e-04\n",
      "[Stage1] Epoch 3 | Batch 400/4152 | Loss: 0.4701 | Batch Acc: 0.939 | LR: 2.32e-04\n",
      "[Stage1] Epoch 3 | Batch 600/4152 | Loss: 0.5037 | Batch Acc: 0.924 | LR: 2.29e-04\n",
      "[Stage1] Epoch 3 | Batch 800/4152 | Loss: 0.4676 | Batch Acc: 0.924 | LR: 2.26e-04\n",
      "[Stage1] Epoch 3 | Batch 1000/4152 | Loss: 0.4877 | Batch Acc: 0.912 | LR: 2.22e-04\n",
      "[Stage1] Epoch 3 | Batch 1200/4152 | Loss: 0.4852 | Batch Acc: 0.924 | LR: 2.19e-04\n",
      "[Stage1] Epoch 3 | Batch 1400/4152 | Loss: 0.4648 | Batch Acc: 0.924 | LR: 2.15e-04\n",
      "[Stage1] Epoch 3 | Batch 1600/4152 | Loss: 0.4651 | Batch Acc: 0.930 | LR: 2.11e-04\n",
      "[Stage1] Epoch 3 | Batch 1800/4152 | Loss: 0.5404 | Batch Acc: 0.893 | LR: 2.08e-04\n",
      "[Stage1] Epoch 3 | Batch 2000/4152 | Loss: 0.4743 | Batch Acc: 0.928 | LR: 2.04e-04\n",
      "[Stage1] Epoch 3 | Batch 2200/4152 | Loss: 0.4736 | Batch Acc: 0.930 | LR: 2.00e-04\n",
      "[Stage1] Epoch 3 | Batch 2400/4152 | Loss: 0.4414 | Batch Acc: 0.926 | LR: 1.97e-04\n",
      "[Stage1] Epoch 3 | Batch 2600/4152 | Loss: 0.4688 | Batch Acc: 0.936 | LR: 1.93e-04\n",
      "[Stage1] Epoch 3 | Batch 2800/4152 | Loss: 0.4445 | Batch Acc: 0.945 | LR: 1.89e-04\n",
      "[Stage1] Epoch 3 | Batch 3000/4152 | Loss: 0.4708 | Batch Acc: 0.930 | LR: 1.85e-04\n",
      "[Stage1] Epoch 3 | Batch 3200/4152 | Loss: 0.4576 | Batch Acc: 0.939 | LR: 1.81e-04\n",
      "[Stage1] Epoch 3 | Batch 3400/4152 | Loss: 0.4686 | Batch Acc: 0.920 | LR: 1.77e-04\n",
      "[Stage1] Epoch 3 | Batch 3600/4152 | Loss: 0.4132 | Batch Acc: 0.959 | LR: 1.73e-04\n",
      "[Stage1] Epoch 3 | Batch 3800/4152 | Loss: 0.4243 | Batch Acc: 0.939 | LR: 1.69e-04\n",
      "[Stage1] Epoch 3 | Batch 4000/4152 | Loss: 0.4732 | Batch Acc: 0.938 | LR: 1.65e-04\n",
      "  [Stage1] Train Loss: 0.4791, Train Acc: 0.9248\n",
      "  [Stage1] Val   Loss: 0.4509, Val   Acc: 0.9280\n",
      "  ✓ [Stage1] New best attack-only model saved (val_acc=0.9280)\n",
      "\n",
      "Epoch 4/6\n",
      "[Stage1] Epoch 4 | Batch 200/4152 | Loss: 0.4849 | Batch Acc: 0.922 | LR: 1.58e-04\n",
      "[Stage1] Epoch 4 | Batch 400/4152 | Loss: 0.4674 | Batch Acc: 0.928 | LR: 1.54e-04\n",
      "[Stage1] Epoch 4 | Batch 600/4152 | Loss: 0.4304 | Batch Acc: 0.947 | LR: 1.50e-04\n",
      "[Stage1] Epoch 4 | Batch 800/4152 | Loss: 0.4494 | Batch Acc: 0.949 | LR: 1.46e-04\n",
      "[Stage1] Epoch 4 | Batch 1000/4152 | Loss: 0.4276 | Batch Acc: 0.949 | LR: 1.42e-04\n",
      "[Stage1] Epoch 4 | Batch 1200/4152 | Loss: 0.5311 | Batch Acc: 0.904 | LR: 1.39e-04\n",
      "[Stage1] Epoch 4 | Batch 1400/4152 | Loss: 0.5079 | Batch Acc: 0.904 | LR: 1.35e-04\n",
      "[Stage1] Epoch 4 | Batch 1600/4152 | Loss: 0.4562 | Batch Acc: 0.936 | LR: 1.31e-04\n",
      "[Stage1] Epoch 4 | Batch 1800/4152 | Loss: 0.4971 | Batch Acc: 0.910 | LR: 1.27e-04\n",
      "[Stage1] Epoch 4 | Batch 2000/4152 | Loss: 0.4654 | Batch Acc: 0.930 | LR: 1.23e-04\n",
      "[Stage1] Epoch 4 | Batch 2200/4152 | Loss: 0.4832 | Batch Acc: 0.920 | LR: 1.19e-04\n",
      "[Stage1] Epoch 4 | Batch 2400/4152 | Loss: 0.4411 | Batch Acc: 0.938 | LR: 1.15e-04\n",
      "[Stage1] Epoch 4 | Batch 2600/4152 | Loss: 0.4251 | Batch Acc: 0.953 | LR: 1.11e-04\n",
      "[Stage1] Epoch 4 | Batch 2800/4152 | Loss: 0.4595 | Batch Acc: 0.939 | LR: 1.07e-04\n",
      "[Stage1] Epoch 4 | Batch 3000/4152 | Loss: 0.4755 | Batch Acc: 0.922 | LR: 1.03e-04\n",
      "[Stage1] Epoch 4 | Batch 3200/4152 | Loss: 0.4466 | Batch Acc: 0.934 | LR: 9.97e-05\n",
      "[Stage1] Epoch 4 | Batch 3400/4152 | Loss: 0.4576 | Batch Acc: 0.938 | LR: 9.59e-05\n",
      "[Stage1] Epoch 4 | Batch 3600/4152 | Loss: 0.4359 | Batch Acc: 0.941 | LR: 9.22e-05\n",
      "[Stage1] Epoch 4 | Batch 3800/4152 | Loss: 0.4619 | Batch Acc: 0.936 | LR: 8.86e-05\n",
      "[Stage1] Epoch 4 | Batch 4000/4152 | Loss: 0.4424 | Batch Acc: 0.941 | LR: 8.50e-05\n",
      "  [Stage1] Train Loss: 0.4563, Train Acc: 0.9341\n",
      "  [Stage1] Val   Loss: 0.4276, Val   Acc: 0.9399\n",
      "  ✓ [Stage1] New best attack-only model saved (val_acc=0.9399)\n",
      "\n",
      "Epoch 5/6\n",
      "[Stage1] Epoch 5 | Batch 200/4152 | Loss: 0.4168 | Batch Acc: 0.941 | LR: 7.87e-05\n",
      "[Stage1] Epoch 5 | Batch 400/4152 | Loss: 0.4430 | Batch Acc: 0.943 | LR: 7.53e-05\n",
      "[Stage1] Epoch 5 | Batch 600/4152 | Loss: 0.4258 | Batch Acc: 0.947 | LR: 7.18e-05\n",
      "[Stage1] Epoch 5 | Batch 800/4152 | Loss: 0.4473 | Batch Acc: 0.938 | LR: 6.85e-05\n",
      "[Stage1] Epoch 5 | Batch 1000/4152 | Loss: 0.4336 | Batch Acc: 0.951 | LR: 6.51e-05\n",
      "[Stage1] Epoch 5 | Batch 1200/4152 | Loss: 0.4220 | Batch Acc: 0.945 | LR: 6.19e-05\n",
      "[Stage1] Epoch 5 | Batch 1400/4152 | Loss: 0.4325 | Batch Acc: 0.953 | LR: 5.87e-05\n",
      "[Stage1] Epoch 5 | Batch 1600/4152 | Loss: 0.4342 | Batch Acc: 0.947 | LR: 5.56e-05\n",
      "[Stage1] Epoch 5 | Batch 1800/4152 | Loss: 0.4634 | Batch Acc: 0.924 | LR: 5.25e-05\n",
      "[Stage1] Epoch 5 | Batch 2000/4152 | Loss: 0.4031 | Batch Acc: 0.957 | LR: 4.95e-05\n",
      "[Stage1] Epoch 5 | Batch 2200/4152 | Loss: 0.4211 | Batch Acc: 0.951 | LR: 4.66e-05\n",
      "[Stage1] Epoch 5 | Batch 2400/4152 | Loss: 0.3950 | Batch Acc: 0.963 | LR: 4.38e-05\n",
      "[Stage1] Epoch 5 | Batch 2600/4152 | Loss: 0.4298 | Batch Acc: 0.938 | LR: 4.10e-05\n",
      "[Stage1] Epoch 5 | Batch 2800/4152 | Loss: 0.4674 | Batch Acc: 0.922 | LR: 3.83e-05\n",
      "[Stage1] Epoch 5 | Batch 3000/4152 | Loss: 0.4172 | Batch Acc: 0.941 | LR: 3.57e-05\n",
      "[Stage1] Epoch 5 | Batch 3200/4152 | Loss: 0.4575 | Batch Acc: 0.939 | LR: 3.31e-05\n",
      "[Stage1] Epoch 5 | Batch 3400/4152 | Loss: 0.4154 | Batch Acc: 0.951 | LR: 3.07e-05\n",
      "[Stage1] Epoch 5 | Batch 3600/4152 | Loss: 0.4446 | Batch Acc: 0.939 | LR: 2.83e-05\n",
      "[Stage1] Epoch 5 | Batch 3800/4152 | Loss: 0.4012 | Batch Acc: 0.959 | LR: 2.60e-05\n",
      "[Stage1] Epoch 5 | Batch 4000/4152 | Loss: 0.4696 | Batch Acc: 0.922 | LR: 2.38e-05\n",
      "  [Stage1] Train Loss: 0.4413, Train Acc: 0.9399\n",
      "  [Stage1] Val   Loss: 0.4117, Val   Acc: 0.9482\n",
      "  ✓ [Stage1] New best attack-only model saved (val_acc=0.9482)\n",
      "\n",
      "Epoch 6/6\n",
      "[Stage1] Epoch 6 | Batch 200/4152 | Loss: 0.4115 | Batch Acc: 0.957 | LR: 2.02e-05\n",
      "[Stage1] Epoch 6 | Batch 400/4152 | Loss: 0.4463 | Batch Acc: 0.932 | LR: 1.82e-05\n",
      "[Stage1] Epoch 6 | Batch 600/4152 | Loss: 0.4363 | Batch Acc: 0.936 | LR: 1.64e-05\n",
      "[Stage1] Epoch 6 | Batch 800/4152 | Loss: 0.4505 | Batch Acc: 0.943 | LR: 1.46e-05\n",
      "[Stage1] Epoch 6 | Batch 1000/4152 | Loss: 0.4364 | Batch Acc: 0.951 | LR: 1.29e-05\n",
      "[Stage1] Epoch 6 | Batch 1200/4152 | Loss: 0.4160 | Batch Acc: 0.959 | LR: 1.14e-05\n",
      "[Stage1] Epoch 6 | Batch 1400/4152 | Loss: 0.4306 | Batch Acc: 0.941 | LR: 9.90e-06\n",
      "[Stage1] Epoch 6 | Batch 1600/4152 | Loss: 0.4705 | Batch Acc: 0.930 | LR: 8.52e-06\n",
      "[Stage1] Epoch 6 | Batch 1800/4152 | Loss: 0.4146 | Batch Acc: 0.953 | LR: 7.25e-06\n",
      "[Stage1] Epoch 6 | Batch 2000/4152 | Loss: 0.4691 | Batch Acc: 0.938 | LR: 6.08e-06\n",
      "[Stage1] Epoch 6 | Batch 2200/4152 | Loss: 0.4017 | Batch Acc: 0.943 | LR: 5.01e-06\n",
      "[Stage1] Epoch 6 | Batch 2400/4152 | Loss: 0.4088 | Batch Acc: 0.957 | LR: 4.04e-06\n",
      "[Stage1] Epoch 6 | Batch 2600/4152 | Loss: 0.4343 | Batch Acc: 0.943 | LR: 3.17e-06\n",
      "[Stage1] Epoch 6 | Batch 2800/4152 | Loss: 0.4053 | Batch Acc: 0.951 | LR: 2.41e-06\n",
      "[Stage1] Epoch 6 | Batch 3000/4152 | Loss: 0.3926 | Batch Acc: 0.963 | LR: 1.75e-06\n",
      "[Stage1] Epoch 6 | Batch 3200/4152 | Loss: 0.4220 | Batch Acc: 0.945 | LR: 1.20e-06\n",
      "[Stage1] Epoch 6 | Batch 3400/4152 | Loss: 0.4277 | Batch Acc: 0.951 | LR: 7.47e-07\n",
      "[Stage1] Epoch 6 | Batch 3600/4152 | Loss: 0.4314 | Batch Acc: 0.943 | LR: 4.02e-07\n",
      "[Stage1] Epoch 6 | Batch 3800/4152 | Loss: 0.4559 | Batch Acc: 0.936 | LR: 1.64e-07\n",
      "[Stage1] Epoch 6 | Batch 4000/4152 | Loss: 0.4514 | Batch Acc: 0.934 | LR: 3.05e-08\n",
      "  [Stage1] Train Loss: 0.4338, Train Acc: 0.9430\n",
      "  [Stage1] Val   Loss: 0.4080, Val   Acc: 0.9481\n",
      "  [Stage1] No improvement for 1 epoch(s)\n",
      "\n",
      "============================================================\n",
      "STAGE 1: Evaluate attack-only model on TEST set\n",
      "============================================================\n",
      "[Stage1] Test Loss: 0.4130\n",
      "[Stage1] Test Accuracy: 0.9478\n",
      "\n",
      "[Stage1] Classification report (attack_id):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.8837    0.9475    0.9145     67500\n",
      " brute force     0.9989    0.9442    0.9708     19722\n",
      "        ddos     0.9950    0.9804    0.9877     67500\n",
      "         dos     0.9838    0.9509    0.9671     67500\n",
      "       mirai     0.9991    0.9823    0.9906     67500\n",
      "       recon     0.9307    0.9371    0.9339     67500\n",
      "    spoofing     0.9742    0.9394    0.9565     67500\n",
      "   web-based     0.7757    0.8393    0.8063     30910\n",
      "\n",
      "    accuracy                         0.9478    455632\n",
      "   macro avg     0.9426    0.9401    0.9409    455632\n",
      "weighted avg     0.9501    0.9478    0.9486    455632\n",
      "\n",
      "\n",
      "============================================================\n",
      "STAGE 2: Device-only training (backbone + attack head frozen)\n",
      "============================================================\n",
      "\n",
      "[Stage2] Epoch 1/6\n",
      "[Stage2] Epoch 1 | Batch 200/4152 | Loss: 4.4816 | Batch Acc: 0.020 | LR: 4.82e-05\n",
      "[Stage2] Epoch 1 | Batch 400/4152 | Loss: 3.7217 | Batch Acc: 0.176 | LR: 9.64e-05\n",
      "[Stage2] Epoch 1 | Batch 600/4152 | Loss: 3.0172 | Batch Acc: 0.285 | LR: 1.45e-04\n",
      "[Stage2] Epoch 1 | Batch 800/4152 | Loss: 2.5986 | Batch Acc: 0.361 | LR: 1.93e-04\n",
      "[Stage2] Epoch 1 | Batch 1000/4152 | Loss: 2.3262 | Batch Acc: 0.408 | LR: 2.41e-04\n",
      "[Stage2] Epoch 1 | Batch 1200/4152 | Loss: 1.9489 | Batch Acc: 0.488 | LR: 2.89e-04\n",
      "[Stage2] Epoch 1 | Batch 1400/4152 | Loss: 2.0179 | Batch Acc: 0.480 | LR: 3.00e-04\n",
      "[Stage2] Epoch 1 | Batch 1600/4152 | Loss: 1.9413 | Batch Acc: 0.482 | LR: 3.00e-04\n",
      "[Stage2] Epoch 1 | Batch 1800/4152 | Loss: 1.7635 | Batch Acc: 0.531 | LR: 3.00e-04\n",
      "[Stage2] Epoch 1 | Batch 2000/4152 | Loss: 1.8248 | Batch Acc: 0.516 | LR: 2.99e-04\n",
      "[Stage2] Epoch 1 | Batch 2200/4152 | Loss: 1.9532 | Batch Acc: 0.477 | LR: 2.99e-04\n",
      "[Stage2] Epoch 1 | Batch 2400/4152 | Loss: 1.8277 | Batch Acc: 0.486 | LR: 2.98e-04\n",
      "[Stage2] Epoch 1 | Batch 2600/4152 | Loss: 1.6721 | Batch Acc: 0.537 | LR: 2.98e-04\n",
      "[Stage2] Epoch 1 | Batch 2800/4152 | Loss: 1.6431 | Batch Acc: 0.525 | LR: 2.97e-04\n",
      "[Stage2] Epoch 1 | Batch 3000/4152 | Loss: 1.5735 | Batch Acc: 0.561 | LR: 2.96e-04\n",
      "[Stage2] Epoch 1 | Batch 3200/4152 | Loss: 1.6666 | Batch Acc: 0.557 | LR: 2.95e-04\n",
      "[Stage2] Epoch 1 | Batch 3400/4152 | Loss: 1.7250 | Batch Acc: 0.512 | LR: 2.94e-04\n",
      "[Stage2] Epoch 1 | Batch 3600/4152 | Loss: 1.5584 | Batch Acc: 0.561 | LR: 2.93e-04\n",
      "[Stage2] Epoch 1 | Batch 3800/4152 | Loss: 1.6391 | Batch Acc: 0.547 | LR: 2.91e-04\n",
      "[Stage2] Epoch 1 | Batch 4000/4152 | Loss: 1.6059 | Batch Acc: 0.545 | LR: 2.90e-04\n",
      "  [Stage2] Train Dev Loss: 2.1905, Train Dev Acc: 0.4388\n",
      "  [Stage2] Val   Att Loss: 0.4111, Val Att Acc: 0.9465\n",
      "  [Stage2] Val   Dev Loss: 1.4837, Val Dev Acc: 0.5787\n",
      "  ✓ [Stage2] New best model saved (Val Dev Acc=0.5787, Val Att Acc=0.9465)\n",
      "\n",
      "[Stage2] Epoch 2/6\n",
      "[Stage2] Epoch 2 | Batch 200/4152 | Loss: 1.6470 | Batch Acc: 0.529 | LR: 2.87e-04\n",
      "[Stage2] Epoch 2 | Batch 400/4152 | Loss: 1.6158 | Batch Acc: 0.529 | LR: 2.86e-04\n",
      "[Stage2] Epoch 2 | Batch 600/4152 | Loss: 1.5713 | Batch Acc: 0.533 | LR: 2.84e-04\n",
      "[Stage2] Epoch 2 | Batch 800/4152 | Loss: 1.5180 | Batch Acc: 0.557 | LR: 2.82e-04\n",
      "[Stage2] Epoch 2 | Batch 1000/4152 | Loss: 1.6797 | Batch Acc: 0.518 | LR: 2.80e-04\n",
      "[Stage2] Epoch 2 | Batch 1200/4152 | Loss: 1.6014 | Batch Acc: 0.551 | LR: 2.78e-04\n",
      "[Stage2] Epoch 2 | Batch 1400/4152 | Loss: 1.6096 | Batch Acc: 0.531 | LR: 2.76e-04\n",
      "[Stage2] Epoch 2 | Batch 1600/4152 | Loss: 1.6181 | Batch Acc: 0.547 | LR: 2.74e-04\n",
      "[Stage2] Epoch 2 | Batch 1800/4152 | Loss: 1.5239 | Batch Acc: 0.562 | LR: 2.72e-04\n",
      "[Stage2] Epoch 2 | Batch 2000/4152 | Loss: 1.5714 | Batch Acc: 0.551 | LR: 2.69e-04\n",
      "[Stage2] Epoch 2 | Batch 2200/4152 | Loss: 1.5946 | Batch Acc: 0.543 | LR: 2.67e-04\n",
      "[Stage2] Epoch 2 | Batch 2400/4152 | Loss: 1.6002 | Batch Acc: 0.539 | LR: 2.64e-04\n",
      "[Stage2] Epoch 2 | Batch 2600/4152 | Loss: 1.6107 | Batch Acc: 0.516 | LR: 2.62e-04\n",
      "[Stage2] Epoch 2 | Batch 2800/4152 | Loss: 1.5759 | Batch Acc: 0.541 | LR: 2.59e-04\n",
      "[Stage2] Epoch 2 | Batch 3000/4152 | Loss: 1.6254 | Batch Acc: 0.551 | LR: 2.56e-04\n",
      "[Stage2] Epoch 2 | Batch 3200/4152 | Loss: 1.5882 | Batch Acc: 0.535 | LR: 2.53e-04\n",
      "[Stage2] Epoch 2 | Batch 3400/4152 | Loss: 1.5179 | Batch Acc: 0.551 | LR: 2.50e-04\n",
      "[Stage2] Epoch 2 | Batch 3600/4152 | Loss: 1.5890 | Batch Acc: 0.555 | LR: 2.47e-04\n",
      "[Stage2] Epoch 2 | Batch 3800/4152 | Loss: 1.4414 | Batch Acc: 0.572 | LR: 2.44e-04\n",
      "[Stage2] Epoch 2 | Batch 4000/4152 | Loss: 1.5209 | Batch Acc: 0.551 | LR: 2.41e-04\n",
      "  [Stage2] Train Dev Loss: 1.5642, Train Dev Acc: 0.5505\n",
      "  [Stage2] Val   Att Loss: 0.4118, Val Att Acc: 0.9466\n",
      "  [Stage2] Val   Dev Loss: 1.3657, Val Dev Acc: 0.5999\n",
      "  ✓ [Stage2] New best model saved (Val Dev Acc=0.5999, Val Att Acc=0.9466)\n",
      "\n",
      "[Stage2] Epoch 3/6\n",
      "[Stage2] Epoch 3 | Batch 200/4152 | Loss: 1.4848 | Batch Acc: 0.549 | LR: 2.36e-04\n",
      "[Stage2] Epoch 3 | Batch 400/4152 | Loss: 1.5706 | Batch Acc: 0.562 | LR: 2.32e-04\n",
      "[Stage2] Epoch 3 | Batch 600/4152 | Loss: 1.5806 | Batch Acc: 0.535 | LR: 2.29e-04\n",
      "[Stage2] Epoch 3 | Batch 800/4152 | Loss: 1.5473 | Batch Acc: 0.549 | LR: 2.26e-04\n",
      "[Stage2] Epoch 3 | Batch 1000/4152 | Loss: 1.4411 | Batch Acc: 0.568 | LR: 2.22e-04\n",
      "[Stage2] Epoch 3 | Batch 1200/4152 | Loss: 1.5431 | Batch Acc: 0.564 | LR: 2.19e-04\n",
      "[Stage2] Epoch 3 | Batch 1400/4152 | Loss: 1.4977 | Batch Acc: 0.543 | LR: 2.15e-04\n",
      "[Stage2] Epoch 3 | Batch 1600/4152 | Loss: 1.5662 | Batch Acc: 0.543 | LR: 2.11e-04\n",
      "[Stage2] Epoch 3 | Batch 1800/4152 | Loss: 1.5229 | Batch Acc: 0.562 | LR: 2.08e-04\n",
      "[Stage2] Epoch 3 | Batch 2000/4152 | Loss: 1.5985 | Batch Acc: 0.523 | LR: 2.04e-04\n",
      "[Stage2] Epoch 3 | Batch 2200/4152 | Loss: 1.5608 | Batch Acc: 0.537 | LR: 2.00e-04\n",
      "[Stage2] Epoch 3 | Batch 2400/4152 | Loss: 1.6363 | Batch Acc: 0.520 | LR: 1.97e-04\n",
      "[Stage2] Epoch 3 | Batch 2600/4152 | Loss: 1.4349 | Batch Acc: 0.561 | LR: 1.93e-04\n",
      "[Stage2] Epoch 3 | Batch 2800/4152 | Loss: 1.3469 | Batch Acc: 0.604 | LR: 1.89e-04\n",
      "[Stage2] Epoch 3 | Batch 3000/4152 | Loss: 1.5049 | Batch Acc: 0.551 | LR: 1.85e-04\n",
      "[Stage2] Epoch 3 | Batch 3200/4152 | Loss: 1.5100 | Batch Acc: 0.559 | LR: 1.81e-04\n",
      "[Stage2] Epoch 3 | Batch 3400/4152 | Loss: 1.5992 | Batch Acc: 0.553 | LR: 1.77e-04\n",
      "[Stage2] Epoch 3 | Batch 3600/4152 | Loss: 1.5032 | Batch Acc: 0.559 | LR: 1.73e-04\n",
      "[Stage2] Epoch 3 | Batch 3800/4152 | Loss: 1.4783 | Batch Acc: 0.568 | LR: 1.69e-04\n",
      "[Stage2] Epoch 3 | Batch 4000/4152 | Loss: 1.5583 | Batch Acc: 0.535 | LR: 1.65e-04\n",
      "  [Stage2] Train Dev Loss: 1.5194, Train Dev Acc: 0.5581\n",
      "  [Stage2] Val   Att Loss: 0.4115, Val Att Acc: 0.9466\n",
      "  [Stage2] Val   Dev Loss: 1.3311, Val Dev Acc: 0.6059\n",
      "  ✓ [Stage2] New best model saved (Val Dev Acc=0.6059, Val Att Acc=0.9466)\n",
      "\n",
      "[Stage2] Epoch 4/6\n",
      "[Stage2] Epoch 4 | Batch 200/4152 | Loss: 1.4886 | Batch Acc: 0.584 | LR: 1.58e-04\n",
      "[Stage2] Epoch 4 | Batch 400/4152 | Loss: 1.5140 | Batch Acc: 0.572 | LR: 1.54e-04\n",
      "[Stage2] Epoch 4 | Batch 600/4152 | Loss: 1.5289 | Batch Acc: 0.551 | LR: 1.50e-04\n",
      "[Stage2] Epoch 4 | Batch 800/4152 | Loss: 1.4397 | Batch Acc: 0.561 | LR: 1.46e-04\n",
      "[Stage2] Epoch 4 | Batch 1000/4152 | Loss: 1.4710 | Batch Acc: 0.576 | LR: 1.42e-04\n",
      "[Stage2] Epoch 4 | Batch 1200/4152 | Loss: 1.4797 | Batch Acc: 0.576 | LR: 1.39e-04\n",
      "[Stage2] Epoch 4 | Batch 1400/4152 | Loss: 1.7074 | Batch Acc: 0.527 | LR: 1.35e-04\n",
      "[Stage2] Epoch 4 | Batch 1600/4152 | Loss: 1.4582 | Batch Acc: 0.561 | LR: 1.31e-04\n",
      "[Stage2] Epoch 4 | Batch 1800/4152 | Loss: 1.4158 | Batch Acc: 0.582 | LR: 1.27e-04\n",
      "[Stage2] Epoch 4 | Batch 2000/4152 | Loss: 1.5009 | Batch Acc: 0.568 | LR: 1.23e-04\n",
      "[Stage2] Epoch 4 | Batch 2200/4152 | Loss: 1.5816 | Batch Acc: 0.525 | LR: 1.19e-04\n",
      "[Stage2] Epoch 4 | Batch 2400/4152 | Loss: 1.5412 | Batch Acc: 0.557 | LR: 1.15e-04\n",
      "[Stage2] Epoch 4 | Batch 2600/4152 | Loss: 1.4592 | Batch Acc: 0.561 | LR: 1.11e-04\n",
      "[Stage2] Epoch 4 | Batch 2800/4152 | Loss: 1.5460 | Batch Acc: 0.574 | LR: 1.07e-04\n",
      "[Stage2] Epoch 4 | Batch 3000/4152 | Loss: 1.4786 | Batch Acc: 0.578 | LR: 1.03e-04\n",
      "[Stage2] Epoch 4 | Batch 3200/4152 | Loss: 1.3977 | Batch Acc: 0.578 | LR: 9.97e-05\n",
      "[Stage2] Epoch 4 | Batch 3400/4152 | Loss: 1.4326 | Batch Acc: 0.592 | LR: 9.59e-05\n",
      "[Stage2] Epoch 4 | Batch 3600/4152 | Loss: 1.4750 | Batch Acc: 0.545 | LR: 9.22e-05\n",
      "[Stage2] Epoch 4 | Batch 3800/4152 | Loss: 1.4958 | Batch Acc: 0.555 | LR: 8.86e-05\n",
      "[Stage2] Epoch 4 | Batch 4000/4152 | Loss: 1.4853 | Batch Acc: 0.578 | LR: 8.50e-05\n",
      "  [Stage2] Train Dev Loss: 1.5049, Train Dev Acc: 0.5608\n",
      "  [Stage2] Val   Att Loss: 0.4112, Val Att Acc: 0.9464\n",
      "  [Stage2] Val   Dev Loss: 1.3176, Val Dev Acc: 0.6077\n",
      "  ✓ [Stage2] New best model saved (Val Dev Acc=0.6077, Val Att Acc=0.9464)\n",
      "\n",
      "[Stage2] Epoch 5/6\n",
      "[Stage2] Epoch 5 | Batch 200/4152 | Loss: 1.5276 | Batch Acc: 0.570 | LR: 7.87e-05\n",
      "[Stage2] Epoch 5 | Batch 400/4152 | Loss: 1.5164 | Batch Acc: 0.561 | LR: 7.53e-05\n",
      "[Stage2] Epoch 5 | Batch 600/4152 | Loss: 1.5328 | Batch Acc: 0.557 | LR: 7.18e-05\n",
      "[Stage2] Epoch 5 | Batch 800/4152 | Loss: 1.5271 | Batch Acc: 0.561 | LR: 6.85e-05\n",
      "[Stage2] Epoch 5 | Batch 1000/4152 | Loss: 1.5325 | Batch Acc: 0.545 | LR: 6.51e-05\n",
      "[Stage2] Epoch 5 | Batch 1200/4152 | Loss: 1.4158 | Batch Acc: 0.586 | LR: 6.19e-05\n",
      "[Stage2] Epoch 5 | Batch 1400/4152 | Loss: 1.3425 | Batch Acc: 0.621 | LR: 5.87e-05\n",
      "[Stage2] Epoch 5 | Batch 1600/4152 | Loss: 1.4228 | Batch Acc: 0.566 | LR: 5.56e-05\n",
      "[Stage2] Epoch 5 | Batch 1800/4152 | Loss: 1.5162 | Batch Acc: 0.570 | LR: 5.25e-05\n",
      "[Stage2] Epoch 5 | Batch 2000/4152 | Loss: 1.5070 | Batch Acc: 0.561 | LR: 4.95e-05\n",
      "[Stage2] Epoch 5 | Batch 2200/4152 | Loss: 1.5277 | Batch Acc: 0.531 | LR: 4.66e-05\n",
      "[Stage2] Epoch 5 | Batch 2400/4152 | Loss: 1.4817 | Batch Acc: 0.564 | LR: 4.38e-05\n",
      "[Stage2] Epoch 5 | Batch 2600/4152 | Loss: 1.5399 | Batch Acc: 0.564 | LR: 4.10e-05\n",
      "[Stage2] Epoch 5 | Batch 2800/4152 | Loss: 1.4145 | Batch Acc: 0.586 | LR: 3.83e-05\n",
      "[Stage2] Epoch 5 | Batch 3000/4152 | Loss: 1.4562 | Batch Acc: 0.553 | LR: 3.57e-05\n",
      "[Stage2] Epoch 5 | Batch 3200/4152 | Loss: 1.4704 | Batch Acc: 0.562 | LR: 3.31e-05\n",
      "[Stage2] Epoch 5 | Batch 3400/4152 | Loss: 1.4278 | Batch Acc: 0.566 | LR: 3.07e-05\n",
      "[Stage2] Epoch 5 | Batch 3600/4152 | Loss: 1.5677 | Batch Acc: 0.545 | LR: 2.83e-05\n",
      "[Stage2] Epoch 5 | Batch 3800/4152 | Loss: 1.4490 | Batch Acc: 0.584 | LR: 2.60e-05\n",
      "[Stage2] Epoch 5 | Batch 4000/4152 | Loss: 1.5572 | Batch Acc: 0.537 | LR: 2.38e-05\n",
      "  [Stage2] Train Dev Loss: 1.5001, Train Dev Acc: 0.5617\n",
      "  [Stage2] Val   Att Loss: 0.4116, Val Att Acc: 0.9461\n",
      "  [Stage2] Val   Dev Loss: 1.3146, Val Dev Acc: 0.6091\n",
      "  ✓ [Stage2] New best model saved (Val Dev Acc=0.6091, Val Att Acc=0.9461)\n",
      "\n",
      "[Stage2] Epoch 6/6\n",
      "[Stage2] Epoch 6 | Batch 200/4152 | Loss: 1.6244 | Batch Acc: 0.568 | LR: 2.02e-05\n",
      "[Stage2] Epoch 6 | Batch 400/4152 | Loss: 1.5110 | Batch Acc: 0.561 | LR: 1.82e-05\n",
      "[Stage2] Epoch 6 | Batch 600/4152 | Loss: 1.5027 | Batch Acc: 0.557 | LR: 1.64e-05\n",
      "[Stage2] Epoch 6 | Batch 800/4152 | Loss: 1.4629 | Batch Acc: 0.561 | LR: 1.46e-05\n",
      "[Stage2] Epoch 6 | Batch 1000/4152 | Loss: 1.5153 | Batch Acc: 0.537 | LR: 1.29e-05\n",
      "[Stage2] Epoch 6 | Batch 1200/4152 | Loss: 1.5377 | Batch Acc: 0.547 | LR: 1.14e-05\n",
      "[Stage2] Epoch 6 | Batch 1400/4152 | Loss: 1.6442 | Batch Acc: 0.533 | LR: 9.90e-06\n",
      "[Stage2] Epoch 6 | Batch 1600/4152 | Loss: 1.4756 | Batch Acc: 0.590 | LR: 8.52e-06\n",
      "[Stage2] Epoch 6 | Batch 1800/4152 | Loss: 1.5167 | Batch Acc: 0.586 | LR: 7.25e-06\n",
      "[Stage2] Epoch 6 | Batch 2000/4152 | Loss: 1.4147 | Batch Acc: 0.578 | LR: 6.08e-06\n",
      "[Stage2] Epoch 6 | Batch 2200/4152 | Loss: 1.6440 | Batch Acc: 0.516 | LR: 5.01e-06\n",
      "[Stage2] Epoch 6 | Batch 2400/4152 | Loss: 1.3964 | Batch Acc: 0.580 | LR: 4.04e-06\n",
      "[Stage2] Epoch 6 | Batch 2600/4152 | Loss: 1.3875 | Batch Acc: 0.586 | LR: 3.17e-06\n",
      "[Stage2] Epoch 6 | Batch 2800/4152 | Loss: 1.5032 | Batch Acc: 0.559 | LR: 2.41e-06\n",
      "[Stage2] Epoch 6 | Batch 3000/4152 | Loss: 1.5777 | Batch Acc: 0.537 | LR: 1.75e-06\n",
      "[Stage2] Epoch 6 | Batch 3200/4152 | Loss: 1.4948 | Batch Acc: 0.562 | LR: 1.20e-06\n",
      "[Stage2] Epoch 6 | Batch 3400/4152 | Loss: 1.6667 | Batch Acc: 0.523 | LR: 7.47e-07\n",
      "[Stage2] Epoch 6 | Batch 3600/4152 | Loss: 1.5340 | Batch Acc: 0.564 | LR: 4.02e-07\n",
      "[Stage2] Epoch 6 | Batch 3800/4152 | Loss: 1.5115 | Batch Acc: 0.547 | LR: 1.64e-07\n",
      "[Stage2] Epoch 6 | Batch 4000/4152 | Loss: 1.3833 | Batch Acc: 0.586 | LR: 3.05e-08\n",
      "  [Stage2] Train Dev Loss: 1.4985, Train Dev Acc: 0.5619\n",
      "  [Stage2] Val   Att Loss: 0.4117, Val Att Acc: 0.9460\n",
      "  [Stage2] Val   Dev Loss: 1.3161, Val Dev Acc: 0.6078\n",
      "  [Stage2] No improvement for 1 epoch(s)\n",
      "\n",
      "============================================================\n",
      "STAGE 2: Evaluate both heads on TEST set\n",
      "============================================================\n",
      "[Stage2] Test Attack Loss: 0.4130, Test Attack Acc: 0.9458\n",
      "[Stage2] Test Device Loss: 1.3119, Test Device Acc: 0.6099\n",
      "\n",
      "[Stage2] Attack head classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.8546    0.9588    0.9037     67500\n",
      " brute force     0.9988    0.9443    0.9708     19722\n",
      "        ddos     0.9958    0.9798    0.9877     67500\n",
      "         dos     0.9875    0.9454    0.9660     67500\n",
      "       mirai     0.9992    0.9822    0.9906     67500\n",
      "       recon     0.9204    0.9411    0.9306     67500\n",
      "    spoofing     0.9720    0.9407    0.9561     67500\n",
      "   web-based     0.8236    0.7865    0.8046     30910\n",
      "\n",
      "    accuracy                         0.9458    455632\n",
      "   macro avg     0.9440    0.9349    0.9388    455632\n",
      "weighted avg     0.9479    0.9458    0.9462    455632\n",
      "\n",
      "\n",
      "[Stage2] Device head classification report (TEST) - Top 20 devices:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "       dc:a6:32:dc:27:d5     0.8108    0.8422    0.8262    109304\n",
      "      Amazon Echo Studio     0.9333    0.8198    0.8729     27359\n",
      "       dc:a6:32:c9:e4:90     0.4173    0.5079    0.4582     26022\n",
      "       dc:a6:32:c9:e4:ab     0.4726    0.6253    0.5384     24025\n",
      "    Arlo Q Indoor Camera     0.9141    0.8883    0.9010     22982\n",
      "       dc:a6:32:c9:e4:d5     0.3173    0.3622    0.3383     17789\n",
      "       e4:5f:01:55:90:c4     0.4178    0.4565    0.4363     17602\n",
      "       ac:17:02:05:34:27     0.9042    0.9622    0.9323     17001\n",
      "      Nest Indoor Camera     0.8724    0.9181    0.8947     14773\n",
      "       3c:18:a0:41:c3:a0     0.6874    0.8778    0.7710     14444\n",
      "       dc:a6:32:c9:e5:ef     0.2659    0.1040    0.1495     13844\n",
      "       56:4f:8a:e1:f3:2d     0.7022    0.7482    0.7245     12166\n",
      "        Amazon Echo Show     0.6755    0.5142    0.5839     11833\n",
      "       dc:a6:32:c9:e5:a4     0.2225    0.0836    0.1215     10423\n",
      "        Amazon Echo Spot     0.6866    0.3964    0.5026      6854\n",
      "          Netatmo Camera     0.7079    0.5950    0.6466      6292\n",
      "Google Nest Mini Speaker     0.7796    0.4619    0.5801      5859\n",
      "             Wyze Camera     0.5955    0.1904    0.2886      5797\n",
      "       Amazon Echo Dot 1     0.7254    0.2188    0.3361      5650\n",
      "       Amazon Echo Dot 2     0.7268    0.7253    0.7260      5435\n",
      "\n",
      "               micro avg     0.6880    0.6743    0.6811    375454\n",
      "               macro avg     0.6418    0.5649    0.5814    375454\n",
      "            weighted avg     0.6827    0.6743    0.6689    375454\n",
      "\n",
      "\n",
      "============================================================\n",
      "STAGE 3: Joint fine-tuning (attack-dominant loss)\n",
      "============================================================\n",
      "\n",
      "[Stage3] Epoch 1/6\n",
      "[Stage3] Epoch 1 | Batch 200/4152 | Loss: 0.5321 (Att: 0.4234, Dev: 1.5105) | LR: 8.03e-07\n",
      "[Stage3] Epoch 1 | Batch 400/4152 | Loss: 0.5682 (Att: 0.4509, Dev: 1.6234) | LR: 1.61e-06\n",
      "[Stage3] Epoch 1 | Batch 600/4152 | Loss: 0.5205 (Att: 0.4197, Dev: 1.4277) | LR: 2.41e-06\n",
      "[Stage3] Epoch 1 | Batch 800/4152 | Loss: 0.5288 (Att: 0.4339, Dev: 1.3834) | LR: 3.21e-06\n",
      "[Stage3] Epoch 1 | Batch 1000/4152 | Loss: 0.4804 (Att: 0.3849, Dev: 1.3392) | LR: 4.01e-06\n",
      "[Stage3] Epoch 1 | Batch 1200/4152 | Loss: 0.5090 (Att: 0.4059, Dev: 1.4363) | LR: 4.82e-06\n",
      "[Stage3] Epoch 1 | Batch 1400/4152 | Loss: 0.5354 (Att: 0.4319, Dev: 1.4671) | LR: 5.62e-06\n",
      "[Stage3] Epoch 1 | Batch 1600/4152 | Loss: 0.5455 (Att: 0.4479, Dev: 1.4241) | LR: 6.42e-06\n",
      "[Stage3] Epoch 1 | Batch 1800/4152 | Loss: 0.5411 (Att: 0.4333, Dev: 1.5114) | LR: 7.23e-06\n",
      "[Stage3] Epoch 1 | Batch 2000/4152 | Loss: 0.5543 (Att: 0.4624, Dev: 1.3807) | LR: 8.03e-06\n",
      "[Stage3] Epoch 1 | Batch 2200/4152 | Loss: 0.5522 (Att: 0.4436, Dev: 1.5289) | LR: 8.83e-06\n",
      "[Stage3] Epoch 1 | Batch 2400/4152 | Loss: 0.5356 (Att: 0.4377, Dev: 1.4164) | LR: 9.63e-06\n",
      "[Stage3] Epoch 1 | Batch 2600/4152 | Loss: 0.5089 (Att: 0.4124, Dev: 1.3772) | LR: 1.00e-05\n",
      "[Stage3] Epoch 1 | Batch 2800/4152 | Loss: 0.5445 (Att: 0.4488, Dev: 1.4056) | LR: 1.00e-05\n",
      "[Stage3] Epoch 1 | Batch 3000/4152 | Loss: 0.5036 (Att: 0.4031, Dev: 1.4083) | LR: 9.99e-06\n",
      "[Stage3] Epoch 1 | Batch 3200/4152 | Loss: 0.5084 (Att: 0.4111, Dev: 1.3840) | LR: 9.98e-06\n",
      "[Stage3] Epoch 1 | Batch 3400/4152 | Loss: 0.5282 (Att: 0.4261, Dev: 1.4467) | LR: 9.96e-06\n",
      "[Stage3] Epoch 1 | Batch 3600/4152 | Loss: 0.5655 (Att: 0.4633, Dev: 1.4851) | LR: 9.94e-06\n",
      "[Stage3] Epoch 1 | Batch 3800/4152 | Loss: 0.5569 (Att: 0.4551, Dev: 1.4734) | LR: 9.92e-06\n",
      "[Stage3] Epoch 1 | Batch 4000/4152 | Loss: 0.4950 (Att: 0.4136, Dev: 1.2276) | LR: 9.89e-06\n",
      "  [Stage3] Train Loss: 0.5350\n",
      "  [Stage3] Val Att Loss: 0.4107, Val Att Acc: 0.9467\n",
      "  [Stage3] Val Dev Loss: 1.2155, Val Dev Acc: 0.6306\n",
      "  [Stage3] Combined metric: 0.8519\n",
      "  ✓ [Stage3] New best joint model saved (combined=0.8519)\n",
      "\n",
      "[Stage3] Epoch 2/6\n",
      "[Stage3] Epoch 2 | Batch 200/4152 | Loss: 0.4671 (Att: 0.3764, Dev: 1.2833) | LR: 9.83e-06\n",
      "[Stage3] Epoch 2 | Batch 400/4152 | Loss: 0.5106 (Att: 0.4272, Dev: 1.2617) | LR: 9.79e-06\n",
      "[Stage3] Epoch 2 | Batch 600/4152 | Loss: 0.5332 (Att: 0.4467, Dev: 1.3116) | LR: 9.75e-06\n",
      "[Stage3] Epoch 2 | Batch 800/4152 | Loss: 0.5491 (Att: 0.4486, Dev: 1.4540) | LR: 9.71e-06\n",
      "[Stage3] Epoch 2 | Batch 1000/4152 | Loss: 0.5419 (Att: 0.4484, Dev: 1.3827) | LR: 9.66e-06\n",
      "[Stage3] Epoch 2 | Batch 1200/4152 | Loss: 0.5023 (Att: 0.4094, Dev: 1.3383) | LR: 9.60e-06\n",
      "[Stage3] Epoch 2 | Batch 1400/4152 | Loss: 0.5356 (Att: 0.4318, Dev: 1.4693) | LR: 9.55e-06\n",
      "[Stage3] Epoch 2 | Batch 1600/4152 | Loss: 0.5122 (Att: 0.4257, Dev: 1.2906) | LR: 9.49e-06\n",
      "[Stage3] Epoch 2 | Batch 1800/4152 | Loss: 0.5053 (Att: 0.4244, Dev: 1.2339) | LR: 9.42e-06\n",
      "[Stage3] Epoch 2 | Batch 2000/4152 | Loss: 0.5290 (Att: 0.4390, Dev: 1.3397) | LR: 9.36e-06\n",
      "[Stage3] Epoch 2 | Batch 2200/4152 | Loss: 0.5180 (Att: 0.4256, Dev: 1.3495) | LR: 9.29e-06\n",
      "[Stage3] Epoch 2 | Batch 2400/4152 | Loss: 0.5339 (Att: 0.4446, Dev: 1.3373) | LR: 9.21e-06\n",
      "[Stage3] Epoch 2 | Batch 2600/4152 | Loss: 0.4939 (Att: 0.4133, Dev: 1.2197) | LR: 9.14e-06\n",
      "[Stage3] Epoch 2 | Batch 2800/4152 | Loss: 0.5365 (Att: 0.4413, Dev: 1.3927) | LR: 9.05e-06\n",
      "[Stage3] Epoch 2 | Batch 3000/4152 | Loss: 0.5229 (Att: 0.4344, Dev: 1.3199) | LR: 8.97e-06\n",
      "[Stage3] Epoch 2 | Batch 3200/4152 | Loss: 0.5276 (Att: 0.4438, Dev: 1.2817) | LR: 8.88e-06\n",
      "[Stage3] Epoch 2 | Batch 3400/4152 | Loss: 0.5417 (Att: 0.4488, Dev: 1.3780) | LR: 8.79e-06\n",
      "[Stage3] Epoch 2 | Batch 3600/4152 | Loss: 0.5320 (Att: 0.4386, Dev: 1.3728) | LR: 8.70e-06\n",
      "[Stage3] Epoch 2 | Batch 3800/4152 | Loss: 0.5328 (Att: 0.4470, Dev: 1.3050) | LR: 8.61e-06\n",
      "[Stage3] Epoch 2 | Batch 4000/4152 | Loss: 0.5189 (Att: 0.4345, Dev: 1.2782) | LR: 8.51e-06\n",
      "  [Stage3] Train Loss: 0.5276\n",
      "  [Stage3] Val Att Loss: 0.4109, Val Att Acc: 0.9473\n",
      "  [Stage3] Val Dev Loss: 1.1769, Val Dev Acc: 0.6416\n",
      "  [Stage3] Combined metric: 0.8556\n",
      "  ✓ [Stage3] New best joint model saved (combined=0.8556)\n",
      "\n",
      "[Stage3] Epoch 3/6\n",
      "[Stage3] Epoch 3 | Batch 200/4152 | Loss: 0.5039 (Att: 0.4189, Dev: 1.2690) | LR: 8.33e-06\n",
      "[Stage3] Epoch 3 | Batch 400/4152 | Loss: 0.5504 (Att: 0.4668, Dev: 1.3030) | LR: 8.22e-06\n",
      "[Stage3] Epoch 3 | Batch 600/4152 | Loss: 0.4937 (Att: 0.4028, Dev: 1.3121) | LR: 8.11e-06\n",
      "[Stage3] Epoch 3 | Batch 800/4152 | Loss: 0.5331 (Att: 0.4499, Dev: 1.2817) | LR: 8.00e-06\n",
      "[Stage3] Epoch 3 | Batch 1000/4152 | Loss: 0.5063 (Att: 0.4153, Dev: 1.3252) | LR: 7.89e-06\n",
      "[Stage3] Epoch 3 | Batch 1200/4152 | Loss: 0.5337 (Att: 0.4462, Dev: 1.3219) | LR: 7.77e-06\n",
      "[Stage3] Epoch 3 | Batch 1400/4152 | Loss: 0.5317 (Att: 0.4395, Dev: 1.3611) | LR: 7.66e-06\n",
      "[Stage3] Epoch 3 | Batch 1600/4152 | Loss: 0.5267 (Att: 0.4271, Dev: 1.4236) | LR: 7.54e-06\n",
      "[Stage3] Epoch 3 | Batch 1800/4152 | Loss: 0.5666 (Att: 0.4779, Dev: 1.3652) | LR: 7.41e-06\n",
      "[Stage3] Epoch 3 | Batch 2000/4152 | Loss: 0.5396 (Att: 0.4514, Dev: 1.3341) | LR: 7.29e-06\n",
      "[Stage3] Epoch 3 | Batch 2200/4152 | Loss: 0.5411 (Att: 0.4489, Dev: 1.3708) | LR: 7.17e-06\n",
      "[Stage3] Epoch 3 | Batch 2400/4152 | Loss: 0.5074 (Att: 0.4141, Dev: 1.3466) | LR: 7.04e-06\n",
      "[Stage3] Epoch 3 | Batch 2600/4152 | Loss: 0.5721 (Att: 0.4723, Dev: 1.4703) | LR: 6.91e-06\n",
      "[Stage3] Epoch 3 | Batch 2800/4152 | Loss: 0.5000 (Att: 0.4207, Dev: 1.2144) | LR: 6.78e-06\n",
      "[Stage3] Epoch 3 | Batch 3000/4152 | Loss: 0.5187 (Att: 0.4252, Dev: 1.3596) | LR: 6.65e-06\n",
      "[Stage3] Epoch 3 | Batch 3200/4152 | Loss: 0.5401 (Att: 0.4507, Dev: 1.3448) | LR: 6.52e-06\n",
      "[Stage3] Epoch 3 | Batch 3400/4152 | Loss: 0.5183 (Att: 0.4319, Dev: 1.2960) | LR: 6.38e-06\n",
      "[Stage3] Epoch 3 | Batch 3600/4152 | Loss: 0.4978 (Att: 0.4171, Dev: 1.2238) | LR: 6.25e-06\n",
      "[Stage3] Epoch 3 | Batch 3800/4152 | Loss: 0.5333 (Att: 0.4440, Dev: 1.3365) | LR: 6.11e-06\n",
      "[Stage3] Epoch 3 | Batch 4000/4152 | Loss: 0.4915 (Att: 0.4021, Dev: 1.2962) | LR: 5.97e-06\n",
      "  [Stage3] Train Loss: 0.5236\n",
      "  [Stage3] Val Att Loss: 0.4092, Val Att Acc: 0.9483\n",
      "  [Stage3] Val Dev Loss: 1.1491, Val Dev Acc: 0.6458\n",
      "  [Stage3] Combined metric: 0.8575\n",
      "  ✓ [Stage3] New best joint model saved (combined=0.8575)\n",
      "\n",
      "[Stage3] Epoch 4/6\n",
      "[Stage3] Epoch 4 | Batch 200/4152 | Loss: 0.5510 (Att: 0.4722, Dev: 1.2606) | LR: 5.73e-06\n",
      "[Stage3] Epoch 4 | Batch 400/4152 | Loss: 0.5196 (Att: 0.4295, Dev: 1.3299) | LR: 5.59e-06\n",
      "[Stage3] Epoch 4 | Batch 600/4152 | Loss: 0.5335 (Att: 0.4545, Dev: 1.2443) | LR: 5.45e-06\n",
      "[Stage3] Epoch 4 | Batch 800/4152 | Loss: 0.5057 (Att: 0.4229, Dev: 1.2508) | LR: 5.31e-06\n",
      "[Stage3] Epoch 4 | Batch 1000/4152 | Loss: 0.5084 (Att: 0.4236, Dev: 1.2715) | LR: 5.17e-06\n",
      "[Stage3] Epoch 4 | Batch 1200/4152 | Loss: 0.5338 (Att: 0.4511, Dev: 1.2782) | LR: 5.03e-06\n",
      "[Stage3] Epoch 4 | Batch 1400/4152 | Loss: 0.4909 (Att: 0.4096, Dev: 1.2225) | LR: 4.89e-06\n",
      "[Stage3] Epoch 4 | Batch 1600/4152 | Loss: 0.5390 (Att: 0.4564, Dev: 1.2829) | LR: 4.75e-06\n",
      "[Stage3] Epoch 4 | Batch 1800/4152 | Loss: 0.5357 (Att: 0.4482, Dev: 1.3223) | LR: 4.61e-06\n",
      "[Stage3] Epoch 4 | Batch 2000/4152 | Loss: 0.5418 (Att: 0.4562, Dev: 1.3125) | LR: 4.47e-06\n",
      "[Stage3] Epoch 4 | Batch 2200/4152 | Loss: 0.5105 (Att: 0.4262, Dev: 1.2696) | LR: 4.33e-06\n",
      "[Stage3] Epoch 4 | Batch 2400/4152 | Loss: 0.5100 (Att: 0.4224, Dev: 1.2982) | LR: 4.19e-06\n",
      "[Stage3] Epoch 4 | Batch 2600/4152 | Loss: 0.5363 (Att: 0.4470, Dev: 1.3402) | LR: 4.06e-06\n",
      "[Stage3] Epoch 4 | Batch 2800/4152 | Loss: 0.4961 (Att: 0.4095, Dev: 1.2752) | LR: 3.92e-06\n",
      "[Stage3] Epoch 4 | Batch 3000/4152 | Loss: 0.5311 (Att: 0.4373, Dev: 1.3750) | LR: 3.78e-06\n",
      "[Stage3] Epoch 4 | Batch 3200/4152 | Loss: 0.4919 (Att: 0.4186, Dev: 1.1514) | LR: 3.65e-06\n",
      "[Stage3] Epoch 4 | Batch 3400/4152 | Loss: 0.5018 (Att: 0.4234, Dev: 1.2077) | LR: 3.51e-06\n",
      "[Stage3] Epoch 4 | Batch 3600/4152 | Loss: 0.5018 (Att: 0.4064, Dev: 1.3601) | LR: 3.38e-06\n",
      "[Stage3] Epoch 4 | Batch 3800/4152 | Loss: 0.5173 (Att: 0.4322, Dev: 1.2832) | LR: 3.25e-06\n",
      "[Stage3] Epoch 4 | Batch 4000/4152 | Loss: 0.5436 (Att: 0.4745, Dev: 1.1653) | LR: 3.12e-06\n",
      "  [Stage3] Train Loss: 0.5213\n",
      "  [Stage3] Val Att Loss: 0.4091, Val Att Acc: 0.9476\n",
      "  [Stage3] Val Dev Loss: 1.1373, Val Dev Acc: 0.6500\n",
      "  [Stage3] Combined metric: 0.8583\n",
      "  ✓ [Stage3] New best joint model saved (combined=0.8583)\n",
      "\n",
      "[Stage3] Epoch 5/6\n",
      "[Stage3] Epoch 5 | Batch 200/4152 | Loss: 0.5143 (Att: 0.4331, Dev: 1.2446) | LR: 2.89e-06\n",
      "[Stage3] Epoch 5 | Batch 400/4152 | Loss: 0.5022 (Att: 0.4136, Dev: 1.2996) | LR: 2.77e-06\n",
      "[Stage3] Epoch 5 | Batch 600/4152 | Loss: 0.5323 (Att: 0.4469, Dev: 1.3007) | LR: 2.64e-06\n",
      "[Stage3] Epoch 5 | Batch 800/4152 | Loss: 0.5279 (Att: 0.4383, Dev: 1.3342) | LR: 2.52e-06\n",
      "[Stage3] Epoch 5 | Batch 1000/4152 | Loss: 0.5035 (Att: 0.4214, Dev: 1.2421) | LR: 2.40e-06\n",
      "[Stage3] Epoch 5 | Batch 1200/4152 | Loss: 0.5445 (Att: 0.4485, Dev: 1.4086) | LR: 2.28e-06\n",
      "[Stage3] Epoch 5 | Batch 1400/4152 | Loss: 0.5190 (Att: 0.4353, Dev: 1.2721) | LR: 2.16e-06\n",
      "[Stage3] Epoch 5 | Batch 1600/4152 | Loss: 0.5296 (Att: 0.4406, Dev: 1.3306) | LR: 2.05e-06\n",
      "[Stage3] Epoch 5 | Batch 1800/4152 | Loss: 0.5390 (Att: 0.4472, Dev: 1.3646) | LR: 1.94e-06\n",
      "[Stage3] Epoch 5 | Batch 2000/4152 | Loss: 0.5223 (Att: 0.4268, Dev: 1.3817) | LR: 1.83e-06\n",
      "[Stage3] Epoch 5 | Batch 2200/4152 | Loss: 0.5334 (Att: 0.4506, Dev: 1.2789) | LR: 1.72e-06\n",
      "[Stage3] Epoch 5 | Batch 2400/4152 | Loss: 0.5356 (Att: 0.4636, Dev: 1.1839) | LR: 1.62e-06\n",
      "[Stage3] Epoch 5 | Batch 2600/4152 | Loss: 0.5170 (Att: 0.4316, Dev: 1.2864) | LR: 1.51e-06\n",
      "[Stage3] Epoch 5 | Batch 2800/4152 | Loss: 0.5087 (Att: 0.4238, Dev: 1.2720) | LR: 1.41e-06\n",
      "[Stage3] Epoch 5 | Batch 3000/4152 | Loss: 0.5147 (Att: 0.4264, Dev: 1.3091) | LR: 1.32e-06\n",
      "[Stage3] Epoch 5 | Batch 3200/4152 | Loss: 0.5186 (Att: 0.4335, Dev: 1.2851) | LR: 1.23e-06\n",
      "[Stage3] Epoch 5 | Batch 3400/4152 | Loss: 0.5701 (Att: 0.4799, Dev: 1.3812) | LR: 1.13e-06\n",
      "[Stage3] Epoch 5 | Batch 3600/4152 | Loss: 0.5630 (Att: 0.4680, Dev: 1.4173) | LR: 1.05e-06\n",
      "[Stage3] Epoch 5 | Batch 3800/4152 | Loss: 0.4935 (Att: 0.4029, Dev: 1.3094) | LR: 9.63e-07\n",
      "[Stage3] Epoch 5 | Batch 4000/4152 | Loss: 0.5327 (Att: 0.4354, Dev: 1.4084) | LR: 8.82e-07\n",
      "  [Stage3] Train Loss: 0.5199\n",
      "  [Stage3] Val Att Loss: 0.4083, Val Att Acc: 0.9481\n",
      "  [Stage3] Val Dev Loss: 1.1310, Val Dev Acc: 0.6512\n",
      "  [Stage3] Combined metric: 0.8591\n",
      "  ✓ [Stage3] New best joint model saved (combined=0.8591)\n",
      "\n",
      "[Stage3] Epoch 6/6\n",
      "[Stage3] Epoch 6 | Batch 200/4152 | Loss: 0.5382 (Att: 0.4522, Dev: 1.3128) | LR: 7.47e-07\n",
      "[Stage3] Epoch 6 | Batch 400/4152 | Loss: 0.5303 (Att: 0.4391, Dev: 1.3509) | LR: 6.75e-07\n",
      "[Stage3] Epoch 6 | Batch 600/4152 | Loss: 0.5420 (Att: 0.4545, Dev: 1.3287) | LR: 6.07e-07\n",
      "[Stage3] Epoch 6 | Batch 800/4152 | Loss: 0.5338 (Att: 0.4433, Dev: 1.3484) | LR: 5.41e-07\n",
      "[Stage3] Epoch 6 | Batch 1000/4152 | Loss: 0.5457 (Att: 0.4580, Dev: 1.3352) | LR: 4.80e-07\n",
      "[Stage3] Epoch 6 | Batch 1200/4152 | Loss: 0.5475 (Att: 0.4519, Dev: 1.4081) | LR: 4.22e-07\n",
      "[Stage3] Epoch 6 | Batch 1400/4152 | Loss: 0.5011 (Att: 0.4097, Dev: 1.3239) | LR: 3.67e-07\n",
      "[Stage3] Epoch 6 | Batch 1600/4152 | Loss: 0.5248 (Att: 0.4432, Dev: 1.2590) | LR: 3.16e-07\n",
      "[Stage3] Epoch 6 | Batch 1800/4152 | Loss: 0.5051 (Att: 0.4221, Dev: 1.2522) | LR: 2.69e-07\n",
      "[Stage3] Epoch 6 | Batch 2000/4152 | Loss: 0.5428 (Att: 0.4576, Dev: 1.3092) | LR: 2.26e-07\n",
      "[Stage3] Epoch 6 | Batch 2200/4152 | Loss: 0.4948 (Att: 0.4120, Dev: 1.2393) | LR: 1.86e-07\n",
      "[Stage3] Epoch 6 | Batch 2400/4152 | Loss: 0.5367 (Att: 0.4469, Dev: 1.3446) | LR: 1.50e-07\n",
      "[Stage3] Epoch 6 | Batch 2600/4152 | Loss: 0.5337 (Att: 0.4367, Dev: 1.4071) | LR: 1.18e-07\n",
      "[Stage3] Epoch 6 | Batch 2800/4152 | Loss: 0.5204 (Att: 0.4349, Dev: 1.2901) | LR: 8.95e-08\n",
      "[Stage3] Epoch 6 | Batch 3000/4152 | Loss: 0.5271 (Att: 0.4445, Dev: 1.2710) | LR: 6.50e-08\n",
      "[Stage3] Epoch 6 | Batch 3200/4152 | Loss: 0.5301 (Att: 0.4380, Dev: 1.3590) | LR: 4.44e-08\n",
      "[Stage3] Epoch 6 | Batch 3400/4152 | Loss: 0.5266 (Att: 0.4375, Dev: 1.3280) | LR: 2.77e-08\n",
      "[Stage3] Epoch 6 | Batch 3600/4152 | Loss: 0.4579 (Att: 0.3836, Dev: 1.1270) | LR: 1.49e-08\n",
      "[Stage3] Epoch 6 | Batch 3800/4152 | Loss: 0.5053 (Att: 0.4269, Dev: 1.2113) | LR: 6.08e-09\n",
      "[Stage3] Epoch 6 | Batch 4000/4152 | Loss: 0.5330 (Att: 0.4405, Dev: 1.3654) | LR: 1.13e-09\n",
      "  [Stage3] Train Loss: 0.5194\n",
      "  [Stage3] Val Att Loss: 0.4081, Val Att Acc: 0.9483\n",
      "  [Stage3] Val Dev Loss: 1.1308, Val Dev Acc: 0.6516\n",
      "  [Stage3] Combined metric: 0.8593\n",
      "  ✓ [Stage3] New best joint model saved (combined=0.8593)\n",
      "\n",
      "============================================================\n",
      "STAGE 3: Final TEST evaluation (joint fine-tuned model)\n",
      "============================================================\n",
      "[Stage3] Test Attack Loss: 0.4094, Test Attack Acc: 0.9480\n",
      "[Stage3] Test Device Loss: 1.1283, Test Device Acc: 0.6517\n",
      "\n",
      "[Stage3] Attack head classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.8743    0.9550    0.9129     67500\n",
      " brute force     0.9988    0.9448    0.9711     19722\n",
      "        ddos     0.9966    0.9798    0.9881     67500\n",
      "         dos     0.9889    0.9448    0.9664     67500\n",
      "       mirai     0.9991    0.9824    0.9906     67500\n",
      "       recon     0.9348    0.9359    0.9353     67500\n",
      "    spoofing     0.9685    0.9441    0.9562     67500\n",
      "   web-based     0.7880    0.8321    0.8094     30910\n",
      "\n",
      "    accuracy                         0.9480    455632\n",
      "   macro avg     0.9436    0.9399    0.9412    455632\n",
      "weighted avg     0.9503    0.9480    0.9487    455632\n",
      "\n",
      "\n",
      "[Stage3] Device head classification report (TEST) - Top 20 devices:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "       dc:a6:32:dc:27:d5     0.8425    0.8531    0.8477    109304\n",
      "      Amazon Echo Studio     0.9242    0.8333    0.8764     27359\n",
      "       dc:a6:32:c9:e4:90     0.5410    0.5085    0.5242     26022\n",
      "       dc:a6:32:c9:e4:ab     0.5627    0.6822    0.6167     24025\n",
      "    Arlo Q Indoor Camera     0.9387    0.9081    0.9232     22982\n",
      "       dc:a6:32:c9:e4:d5     0.4276    0.4744    0.4498     17789\n",
      "       e4:5f:01:55:90:c4     0.4913    0.5202    0.5053     17602\n",
      "       ac:17:02:05:34:27     0.9285    0.9734    0.9504     17001\n",
      "      Nest Indoor Camera     0.9433    0.9590    0.9511     14773\n",
      "       3c:18:a0:41:c3:a0     0.7418    0.9022    0.8142     14444\n",
      "       dc:a6:32:c9:e5:ef     0.3103    0.2818    0.2954     13844\n",
      "       56:4f:8a:e1:f3:2d     0.7468    0.8050    0.7748     12166\n",
      "        Amazon Echo Show     0.7227    0.6450    0.6816     11833\n",
      "       dc:a6:32:c9:e5:a4     0.2705    0.1712    0.2096     10423\n",
      "        Amazon Echo Spot     0.7062    0.4732    0.5667      6854\n",
      "          Netatmo Camera     0.7857    0.7161    0.7493      6292\n",
      "Google Nest Mini Speaker     0.8074    0.5416    0.6483      5859\n",
      "             Wyze Camera     0.5993    0.3478    0.4401      5797\n",
      "       Amazon Echo Dot 1     0.6783    0.2586    0.3744      5650\n",
      "       Amazon Echo Dot 2     0.7525    0.7087    0.7300      5435\n",
      "\n",
      "               micro avg     0.7323    0.7171    0.7246    375454\n",
      "               macro avg     0.6861    0.6282    0.6465    375454\n",
      "            weighted avg     0.7295    0.7171    0.7191    375454\n",
      "\n",
      "\n",
      "============================================================\n",
      "Multitask CNN+MLP staged training complete.\n",
      "============================================================\n",
      "Models saved in: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/models\n",
      "Histories and reports in: /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/reports\n"
     ]
    }
   ],
   "source": [
    "# --- Script: 07_multitask_cnn_staged.py ---\n",
    "# Goal:\n",
    "# - Multitask model with shared CNN backbone:\n",
    "#     Task 1: Intrusion Detection (attack_id, 8 classes)\n",
    "#     Task 2: Device Identification (device_id, 94 classes)\n",
    "# - Stage 1: Train attack head only (single-task, high accuracy)\n",
    "# - Stage 2: Freeze backbone+attack head, train device head only\n",
    "# - Stage 3  small joint fine-tuning (attack-dominant loss)\n",
    "\n",
    "# ============================================================\n",
    "# 0. Environment & paths\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[0]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Using processed data from:\", PROCESSED_DIR)\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# 1. Config\n",
    "# ============================================================\n",
    "\n",
    "TRAIN_PATH = PROCESSED_DIR / \"packets_train.csv\"\n",
    "VAL_PATH   = PROCESSED_DIR / \"packets_val.csv\"\n",
    "TEST_PATH  = PROCESSED_DIR / \"packets_test.csv\"\n",
    "\n",
    "ATTACK_LABEL_MAP_PATH = PROCESSED_DIR / \"attack_label_mapping.json\"\n",
    "DEVICE_LABEL_MAP_PATH = PROCESSED_DIR / \"device_label_mapping.json\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Slightly longer training to stabilise backbone\n",
    "NUM_EPOCHS_STAGE1 = 6   # attack-only training\n",
    "NUM_EPOCHS_STAGE2 = 6   # device-only training\n",
    "NUM_EPOCHS_STAGE3 = 6   # optional joint fine-tune\n",
    "\n",
    "LEARNING_RATE_STAGE1 = 3e-4\n",
    "LEARNING_RATE_STAGE2 = 3e-4\n",
    "LEARNING_RATE_STAGE3 = 1e-5  # much smaller for fine-tuning\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_GRAD_NORM = 1.0\n",
    "PATIENCE_STAGE1 = 5\n",
    "PATIENCE_STAGE2 = 5\n",
    "PATIENCE_STAGE3 = 3\n",
    "\n",
    "# Joint fine-tune flag\n",
    "DO_JOINT_FINETUNE = True\n",
    "\n",
    "# Loss weights in joint phase: attack dominates\n",
    "LAMBDA_ATTACK_JOINT = 0.9\n",
    "LAMBDA_DEVICE_JOINT = 0.1\n",
    "\n",
    "# Validation selection in Stage 3: bias to attack performance\n",
    "JOINT_METRIC_ATTACK_WEIGHT = 0.7\n",
    "JOINT_METRIC_DEVICE_WEIGHT = 0.3\n",
    "\n",
    "# Label smoothing for attack loss (helps stability slightly)\n",
    "ATTACK_LABEL_SMOOTH = 0.05\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load data\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading processed CSVs...\")\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df   = pd.read_csv(VAL_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Val   shape:\", val_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "\n",
    "with open(ATTACK_LABEL_MAP_PATH, \"r\") as f:\n",
    "    attack_label_mapping = json.load(f)[\"id_to_attack\"]\n",
    "with open(DEVICE_LABEL_MAP_PATH, \"r\") as f:\n",
    "    device_label_mapping = json.load(f)[\"id_to_device\"]\n",
    "\n",
    "num_attacks = len(attack_label_mapping)\n",
    "num_devices = len(device_label_mapping)\n",
    "print(\"Number of attack classes:\", num_attacks)\n",
    "print(\"Number of device classes:\", num_devices)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Feature selection\n",
    "# ============================================================\n",
    "\n",
    "TARGET_ATTACK = \"attack_id\"\n",
    "TARGET_DEVICE = \"device_id\"\n",
    "\n",
    "numeric_cols = train_df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c not in [TARGET_ATTACK, TARGET_DEVICE]]\n",
    "\n",
    "print(\"\\nNumber of feature columns:\", len(feature_cols))\n",
    "print(\"Example features:\", feature_cols[:15])\n",
    "\n",
    "if TARGET_ATTACK not in train_df.columns or TARGET_DEVICE not in train_df.columns:\n",
    "    raise ValueError(\"attack_id/device_id target columns not found in train_df.\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. Robust preprocessing: NaN/Inf cleaning + standardisation\n",
    "# ============================================================\n",
    "\n",
    "def clean_df(df, feature_cols, name):\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    nan_before = df[feature_cols].isna().sum().sum()\n",
    "    if nan_before > 0:\n",
    "        print(f\"  [{name}] NaN before fill: {nan_before}, filling with 0.\")\n",
    "        df[feature_cols] = df[feature_cols].fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"\\nCleaning NaN/Inf...\")\n",
    "train_df = clean_df(train_df, feature_cols, \"train\")\n",
    "val_df   = clean_df(val_df, feature_cols, \"val\")\n",
    "test_df  = clean_df(test_df, feature_cols, \"test\")\n",
    "\n",
    "print(\"\\nStandardising features...\")\n",
    "means = train_df[feature_cols].mean()\n",
    "stds  = train_df[feature_cols].std().replace(0, 1.0)\n",
    "\n",
    "for df, name in [(train_df, \"train\"), (val_df, \"val\"), (test_df, \"test\")]:\n",
    "    df[feature_cols] = (df[feature_cols] - means) / stds\n",
    "    df[feature_cols] = df[feature_cols].clip(-10, 10)\n",
    "    n_nan = df[feature_cols].isna().sum().sum()\n",
    "    n_inf = np.isinf(df[feature_cols].values).sum()\n",
    "    print(f\"  [{name}] NaN after std: {n_nan}, Inf: {n_inf}\")\n",
    "    if n_nan > 0 or n_inf > 0:\n",
    "        raise ValueError(f\"Found NaN/Inf in {name} after standardisation.\")\n",
    "\n",
    "# Save scaler (useful for deployment)\n",
    "scaler_path = PROCESSED_DIR / \"multitask_cnn_mlp_scaler.json\"\n",
    "with open(scaler_path, \"w\") as f:\n",
    "    json.dump({\"means\": means.to_dict(), \"stds\": stds.to_dict()}, f, indent=2)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Dataset / DataLoader\n",
    "# ============================================================\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols, attack_col=\"attack_id\", device_col=\"device_id\"):\n",
    "        self.X = df[feature_cols].values.astype(np.float32)\n",
    "        self.y_attack = df[attack_col].values.astype(np.int64)\n",
    "        self.y_device = df[device_col].values.astype(np.int64)\n",
    "        assert not np.isnan(self.X).any()\n",
    "        assert not np.isinf(self.X).any()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_attack[idx], self.y_device[idx]\n",
    "\n",
    "train_dataset = MultiTaskDataset(train_df, feature_cols)\n",
    "val_dataset   = MultiTaskDataset(val_df, feature_cols)\n",
    "test_dataset  = MultiTaskDataset(test_df, feature_cols)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          drop_last=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          drop_last=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          drop_last=False, num_workers=0)\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(\"  Train:\", len(train_dataset))\n",
    "print(\"  Val  :\", len(val_dataset))\n",
    "print(\"  Test :\", len(test_dataset))\n",
    "\n",
    "# ============================================================\n",
    "# 6. Model: CNN+MLP backbone + two heads\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "# 6. Model: 1D CNN backbone + two heads\n",
    "# ============================================================\n",
    "\n",
    "class CNNBackbone1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared CNN backbone:\n",
    "\n",
    "    Input:\n",
    "        x: (B, F)  where F = num_features\n",
    "\n",
    "    Steps:\n",
    "        - Treat feature dimension as a 1D sequence.\n",
    "        - Apply several Conv1d + BN + ReLU blocks.\n",
    "        - Global average pooling over sequence length.\n",
    "        - Small linear layer to produce shared representation.\n",
    "\n",
    "    Output:\n",
    "        rep: (B, rep_dim)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        conv_channels: int = 64,\n",
    "        conv_channels_mid: int = 128,\n",
    "        rep_dim: int = 128,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.rep_dim = rep_dim\n",
    "\n",
    "        # x comes as (B, F) -> (B, 1, F)\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(1, conv_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(conv_channels, conv_channels_mid, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv_channels_mid),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # A smaller kernel to refine local interactions\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv1d(conv_channels_mid, conv_channels_mid, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(conv_channels_mid),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Optional downsampling (not strictly required, but can help)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Global average pooling over the remaining feature dimension\n",
    "        # then project to rep_dim\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(conv_channels_mid, rep_dim),\n",
    "            nn.BatchNorm1d(rep_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, F)\n",
    "        x = x.unsqueeze(1)  # (B, 1, F)\n",
    "\n",
    "        x = self.conv_block1(x)          # (B, C1, F)\n",
    "        x = self.conv_block2(x)          # (B, C2, F)\n",
    "        x = self.conv_block3(x)          # (B, C2, F)\n",
    "        x = self.pool(x)                 # (B, C2, F/2)\n",
    "\n",
    "        # Global average over sequence length (last dim)\n",
    "        x = x.mean(dim=2)                # (B, C2)\n",
    "\n",
    "        # Project to shared representation\n",
    "        x = self.proj(x)                 # (B, rep_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiTaskCNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Multitask model with shared CNN backbone and two classification heads:\n",
    "        - attack_id head\n",
    "        - device_id head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        num_attacks: int,\n",
    "        num_devices: int,\n",
    "        conv_channels: int = 64,\n",
    "        conv_channels_mid: int = 128,\n",
    "        rep_dim: int = 128,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = CNNBackbone1D(\n",
    "            num_features=num_features,\n",
    "            conv_channels=conv_channels,\n",
    "            conv_channels_mid=conv_channels_mid,\n",
    "            rep_dim=rep_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.attack_head = nn.Linear(rep_dim, num_attacks)\n",
    "        self.device_head = nn.Linear(rep_dim, num_devices)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.attack_head.weight, nonlinearity=\"linear\")\n",
    "        nn.init.constant_(self.attack_head.bias, 0.0)\n",
    "        nn.init.kaiming_normal_(self.device_head.weight, nonlinearity=\"linear\")\n",
    "        nn.init.constant_(self.device_head.bias, 0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        rep = self.backbone(x)\n",
    "        logits_attack = self.attack_head(rep)\n",
    "        logits_device = self.device_head(rep)\n",
    "        return logits_attack, logits_device\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "num_features = len(feature_cols)\n",
    "print(\"\\nBuilding MultiTask 1D-CNN model:\")\n",
    "print(\"  num_features:\", num_features)\n",
    "print(\"  num_attacks :\", num_attacks)\n",
    "print(\"  num_devices :\", num_devices)\n",
    "\n",
    "model = MultiTaskCNN1D(\n",
    "    num_features=num_features,\n",
    "    num_attacks=num_attacks,\n",
    "    num_devices=num_devices,\n",
    "    conv_channels=64,\n",
    "    conv_channels_mid=128,\n",
    "    rep_dim=128,\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "print(\"Total parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# sanity check\n",
    "x_dummy = torch.randn(8, num_features, device=device)\n",
    "with torch.no_grad():\n",
    "    logits_att_dummy, logits_dev_dummy = model(x_dummy)\n",
    "    print(\"Dummy attack logits shape:\", logits_att_dummy.shape)\n",
    "    print(\"Dummy device logits shape:\", logits_dev_dummy.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 7. Scheduler helper\n",
    "# ============================================================\n",
    "\n",
    "def get_warmup_cosine_schedule(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# ============================================================\n",
    "# 8. Stage 1: attack-only training\n",
    "# ============================================================\n",
    "\n",
    "criterion_attack = nn.CrossEntropyLoss(label_smoothing=ATTACK_LABEL_SMOOTH)\n",
    "criterion_device = nn.CrossEntropyLoss()  # used later\n",
    "\n",
    "def train_epoch_attack_only(model, loader, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y_att, y_dev) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y_att = y_att.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_att, _ = model(x)\n",
    "        loss = criterion_attack(logits_att, y_att)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = logits_att.argmax(dim=1)\n",
    "        total_correct += (preds == y_att).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            batch_acc = (preds == y_att).float().mean().item()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"[Stage1] Epoch {epoch} | Batch {batch_idx+1}/{len(loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.3f} | LR: {lr:.2e}\")\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "def eval_attack_only(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_y = []\n",
    "    all_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y_att, y_dev in loader:\n",
    "            x = x.to(device)\n",
    "            y_att = y_att.to(device)\n",
    "\n",
    "            logits_att, _ = model(x)\n",
    "            loss = criterion_attack(logits_att, y_att)\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = logits_att.argmax(dim=1)\n",
    "            total_correct += (preds == y_att).sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "            all_y.append(y_att.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    y_true = np.concatenate(all_y)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "\n",
    "    return avg_loss, avg_acc, y_true, y_pred\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 1: Attack-only training (single-task)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "optimizer_stage1 = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE_STAGE1,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "total_steps_stage1 = len(train_loader) * NUM_EPOCHS_STAGE1\n",
    "warmup_steps_stage1 = int(0.05 * total_steps_stage1)\n",
    "scheduler_stage1 = get_warmup_cosine_schedule(optimizer_stage1, warmup_steps_stage1, total_steps_stage1)\n",
    "\n",
    "best_val_acc_stage1 = 0.0\n",
    "epochs_no_improve1 = 0\n",
    "history_stage1 = []\n",
    "best_attack_model_path = MODELS_DIR / \"multitask_cnn_mlp_stage1_attack_pretrained.pt\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS_STAGE1 + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS_STAGE1}\")\n",
    "    train_loss, train_acc = train_epoch_attack_only(model, train_loader, optimizer_stage1, scheduler_stage1, epoch)\n",
    "    val_loss, val_acc, y_val_true, y_val_pred = eval_attack_only(model, val_loader)\n",
    "\n",
    "    history_stage1.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"lr\": scheduler_stage1.get_last_lr()[0],\n",
    "    })\n",
    "\n",
    "    print(f\"  [Stage1] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  [Stage1] Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc_stage1 + 1e-4:\n",
    "        best_val_acc_stage1 = val_acc\n",
    "        epochs_no_improve1 = 0\n",
    "        torch.save(model.state_dict(), best_attack_model_path)\n",
    "        print(f\"  ✓ [Stage1] New best attack-only model saved (val_acc={val_acc:.4f})\")\n",
    "    else:\n",
    "        epochs_no_improve1 += 1\n",
    "        print(f\"  [Stage1] No improvement for {epochs_no_improve1} epoch(s)\")\n",
    "\n",
    "    if epochs_no_improve1 >= PATIENCE_STAGE1:\n",
    "        print(\"\\n[Stage1] Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "pd.DataFrame(history_stage1).to_csv(REPORTS_DIR / \"multitask_stage1_attack_history.csv\", index=False)\n",
    "\n",
    "# Load best attack-only weights before Stage 2\n",
    "model.load_state_dict(torch.load(best_attack_model_path, map_location=device))\n",
    "\n",
    "# Evaluate attack head on TEST after Stage 1\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 1: Evaluate attack-only model on TEST set\")\n",
    "print(\"=\"*60)\n",
    "test_loss1, test_acc1, y_test_true1, y_test_pred1 = eval_attack_only(model, test_loader)\n",
    "print(f\"[Stage1] Test Loss: {test_loss1:.4f}\")\n",
    "print(f\"[Stage1] Test Accuracy: {test_acc1:.4f}\")\n",
    "\n",
    "attack_names = [attack_label_mapping[str(i)] for i in range(num_attacks)]\n",
    "stage1_report = classification_report(\n",
    "    y_test_true1,\n",
    "    y_test_pred1,\n",
    "    target_names=attack_names,\n",
    "    digits=4,\n",
    "    zero_division=0,\n",
    ")\n",
    "print(\"\\n[Stage1] Classification report (attack_id):\")\n",
    "print(stage1_report)\n",
    "\n",
    "with open(REPORTS_DIR / \"multitask_stage1_attack_test_report.txt\", \"w\") as f:\n",
    "    f.write(stage1_report)\n",
    "    f.write(\"\\n\\nConfusion matrix:\\n\")\n",
    "    f.write(str(confusion_matrix(y_test_true1, y_test_pred1)))\n",
    "\n",
    "# ============================================================\n",
    "# 9. Stage 2: freeze backbone+attack head, train device head only\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: Device-only training (backbone + attack head frozen)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Freeze backbone and attack head\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.attack_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer only on device head\n",
    "optimizer_stage2 = torch.optim.AdamW(\n",
    "    model.device_head.parameters(),\n",
    "    lr=LEARNING_RATE_STAGE2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "total_steps_stage2 = len(train_loader) * NUM_EPOCHS_STAGE2\n",
    "warmup_steps_stage2 = int(0.05 * total_steps_stage2)\n",
    "scheduler_stage2 = get_warmup_cosine_schedule(optimizer_stage2, warmup_steps_stage2, total_steps_stage2)\n",
    "\n",
    "def train_epoch_device_only(model, loader, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y_att, y_dev) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y_dev = y_dev.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, logits_dev = model(x)\n",
    "        loss = criterion_device(logits_dev, y_dev)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.device_head.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = logits_dev.argmax(dim=1)\n",
    "        total_correct += (preds == y_dev).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            batch_acc = (preds == y_dev).float().mean().item()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"[Stage2] Epoch {epoch} | Batch {batch_idx+1}/{len(loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.3f} | LR: {lr:.2e}\")\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "def eval_both_heads(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate both attack and device heads (no gradients).\n",
    "    Used in Stage 2 & 3 to see if attack performance is preserved.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss_att = 0.0\n",
    "    total_loss_dev = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_att_true, all_att_pred = [], []\n",
    "    all_dev_true, all_dev_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y_att, y_dev in loader:\n",
    "            x = x.to(device)\n",
    "            y_att = y_att.to(device)\n",
    "            y_dev = y_dev.to(device)\n",
    "\n",
    "            logits_att, logits_dev = model(x)\n",
    "            loss_att = criterion_attack(logits_att, y_att)\n",
    "            loss_dev = criterion_device(logits_dev, y_dev)\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            total_loss_att += loss_att.item() * batch_size\n",
    "            total_loss_dev += loss_dev.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            pred_att = logits_att.argmax(dim=1)\n",
    "            pred_dev = logits_dev.argmax(dim=1)\n",
    "\n",
    "            all_att_true.append(y_att.cpu().numpy())\n",
    "            all_att_pred.append(pred_att.cpu().numpy())\n",
    "            all_dev_true.append(y_dev.cpu().numpy())\n",
    "            all_dev_pred.append(pred_dev.cpu().numpy())\n",
    "\n",
    "    avg_loss_att = total_loss_att / total_samples\n",
    "    avg_loss_dev = total_loss_dev / total_samples\n",
    "\n",
    "    y_att_true = np.concatenate(all_att_true)\n",
    "    y_att_pred = np.concatenate(all_att_pred)\n",
    "    y_dev_true = np.concatenate(all_dev_true)\n",
    "    y_dev_pred = np.concatenate(all_dev_pred)\n",
    "\n",
    "    att_acc = (y_att_true == y_att_pred).mean()\n",
    "    dev_acc = (y_dev_true == y_dev_pred).mean()\n",
    "\n",
    "    return avg_loss_att, avg_loss_dev, att_acc, dev_acc, y_att_true, y_att_pred, y_dev_true, y_dev_pred\n",
    "\n",
    "\n",
    "best_dev_acc_stage2 = 0.0\n",
    "epochs_no_improve2 = 0\n",
    "history_stage2 = []\n",
    "best_stage2_model_path = MODELS_DIR / \"multitask_cnn_mlp_stage2_device_trained.pt\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS_STAGE2 + 1):\n",
    "    print(f\"\\n[Stage2] Epoch {epoch}/{NUM_EPOCHS_STAGE2}\")\n",
    "    train_loss_dev, train_acc_dev = train_epoch_device_only(model, train_loader, optimizer_stage2, scheduler_stage2, epoch)\n",
    "    val_loss_att2, val_loss_dev2, val_att_acc2, val_dev_acc2, _, _, _, _ = eval_both_heads(model, val_loader)\n",
    "\n",
    "    history_stage2.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss_dev\": train_loss_dev,\n",
    "        \"train_acc_dev\": train_acc_dev,\n",
    "        \"val_loss_att\": val_loss_att2,\n",
    "        \"val_loss_dev\": val_loss_dev2,\n",
    "        \"val_att_acc\": val_att_acc2,\n",
    "        \"val_dev_acc\": val_dev_acc2,\n",
    "        \"lr\": scheduler_stage2.get_last_lr()[0],\n",
    "    })\n",
    "\n",
    "    print(f\"  [Stage2] Train Dev Loss: {train_loss_dev:.4f}, Train Dev Acc: {train_acc_dev:.4f}\")\n",
    "    print(f\"  [Stage2] Val   Att Loss: {val_loss_att2:.4f}, Val Att Acc: {val_att_acc2:.4f}\")\n",
    "    print(f\"  [Stage2] Val   Dev Loss: {val_loss_dev2:.4f}, Val Dev Acc: {val_dev_acc2:.4f}\")\n",
    "\n",
    "    # Track best by device accuracy, but keep an eye on attack acc\n",
    "    if val_dev_acc2 > best_dev_acc_stage2 + 1e-4:\n",
    "        best_dev_acc_stage2 = val_dev_acc2\n",
    "        epochs_no_improve2 = 0\n",
    "        torch.save(model.state_dict(), best_stage2_model_path)\n",
    "        print(f\"  ✓ [Stage2] New best model saved (Val Dev Acc={val_dev_acc2:.4f}, Val Att Acc={val_att_acc2:.4f})\")\n",
    "    else:\n",
    "        epochs_no_improve2 += 1\n",
    "        print(f\"  [Stage2] No improvement for {epochs_no_improve2} epoch(s)\")\n",
    "\n",
    "    if epochs_no_improve2 >= PATIENCE_STAGE2:\n",
    "        print(\"\\n[Stage2] Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "pd.DataFrame(history_stage2).to_csv(REPORTS_DIR / \"multitask_stage2_device_history.csv\", index=False)\n",
    "\n",
    "# Load best Stage 2 weights\n",
    "model.load_state_dict(torch.load(best_stage2_model_path, map_location=device))\n",
    "\n",
    "# Evaluate both heads on TEST after Stage 2\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: Evaluate both heads on TEST set\")\n",
    "print(\"=\"*60)\n",
    "test_loss_att2, test_loss_dev2, test_att_acc2, test_dev_acc2, y_att_true2, y_att_pred2, y_dev_true2, y_dev_pred2 = eval_both_heads(model, test_loader)\n",
    "print(f\"[Stage2] Test Attack Loss: {test_loss_att2:.4f}, Test Attack Acc: {test_att_acc2:.4f}\")\n",
    "print(f\"[Stage2] Test Device Loss: {test_loss_dev2:.4f}, Test Device Acc: {test_dev_acc2:.4f}\")\n",
    "\n",
    "print(\"\\n[Stage2] Attack head classification report (TEST):\")\n",
    "stage2_attack_report = classification_report(\n",
    "    y_att_true2,\n",
    "    y_att_pred2,\n",
    "    target_names=attack_names,\n",
    "    digits=4,\n",
    "    zero_division=0,\n",
    ")\n",
    "print(stage2_attack_report)\n",
    "\n",
    "# Device: top-20 frequent devices only (to keep report readable)\n",
    "dev_counts = pd.Series(y_dev_true2).value_counts()\n",
    "top_k = min(20, len(dev_counts))\n",
    "top_dev_ids = dev_counts.index[:top_k]\n",
    "mask_top = np.isin(y_dev_true2, top_dev_ids)\n",
    "y_dev_true_top = y_dev_true2[mask_top]\n",
    "y_dev_pred_top = y_dev_pred2[mask_top]\n",
    "top_dev_names = [device_label_mapping[str(i)] for i in top_dev_ids]\n",
    "\n",
    "print(\"\\n[Stage2] Device head classification report (TEST) - Top 20 devices:\")\n",
    "stage2_device_report = classification_report(\n",
    "    y_dev_true_top,\n",
    "    y_dev_pred_top,\n",
    "    labels=top_dev_ids,              # ensure labels size matches target_names\n",
    "    target_names=top_dev_names,\n",
    "    digits=4,\n",
    "    zero_division=0,\n",
    ")\n",
    "print(stage2_device_report)\n",
    "\n",
    "with open(REPORTS_DIR / \"multitask_stage2_test_reports.txt\", \"w\") as f:\n",
    "    f.write(\"Attack head report (TEST):\\n\")\n",
    "    f.write(stage2_attack_report)\n",
    "    f.write(\"\\n\\nDevice head report (TEST) - Top 20 devices:\\n\")\n",
    "    f.write(stage2_device_report)\n",
    "\n",
    "# ============================================================\n",
    "# 10. Stage 3 (optional): joint fine-tuning\n",
    "# ============================================================\n",
    "\n",
    "if DO_JOINT_FINETUNE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STAGE 3: Joint fine-tuning (attack-dominant loss)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Unfreeze entire model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer_stage3 = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE_STAGE3,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    total_steps_stage3 = len(train_loader) * NUM_EPOCHS_STAGE3\n",
    "    warmup_steps_stage3 = int(0.1 * total_steps_stage3)  # small warmup\n",
    "    scheduler_stage3 = get_warmup_cosine_schedule(optimizer_stage3, warmup_steps_stage3, total_steps_stage3)\n",
    "\n",
    "    best_combined_stage3 = -1.0\n",
    "    epochs_no_improve3 = 0\n",
    "    history_stage3 = []\n",
    "    best_stage3_model_path = MODELS_DIR / \"multitask_cnn_mlp_stage3_joint_finetuned.pt\"\n",
    "\n",
    "    def train_epoch_joint(model, loader, optimizer, scheduler, epoch):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (x, y_att, y_dev) in enumerate(loader):\n",
    "            x = x.to(device)\n",
    "            y_att = y_att.to(device)\n",
    "            y_dev = y_dev.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_att, logits_dev = model(x)\n",
    "            loss_att = criterion_attack(logits_att, y_att)\n",
    "            loss_dev = criterion_device(logits_dev, y_dev)\n",
    "            loss = LAMBDA_ATTACK_JOINT * loss_att + LAMBDA_DEVICE_JOINT * loss_dev\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                print(f\"[Stage3] Epoch {epoch} | Batch {batch_idx+1}/{len(loader)} | \"\n",
    "                      f\"Loss: {loss.item():.4f} (Att: {loss_att.item():.4f}, Dev: {loss_dev.item():.4f}) | LR: {lr:.2e}\")\n",
    "\n",
    "        return total_loss / total_samples\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS_STAGE3 + 1):\n",
    "        print(f\"\\n[Stage3] Epoch {epoch}/{NUM_EPOCHS_STAGE3}\")\n",
    "        train_loss3 = train_epoch_joint(model, train_loader, optimizer_stage3, scheduler_stage3, epoch)\n",
    "        val_loss_att3, val_loss_dev3, val_att_acc3, val_dev_acc3, _, _, _, _ = eval_both_heads(model, val_loader)\n",
    "\n",
    "        combined_metric = (\n",
    "            JOINT_METRIC_ATTACK_WEIGHT * val_att_acc3\n",
    "            + JOINT_METRIC_DEVICE_WEIGHT * val_dev_acc3\n",
    "        )\n",
    "        history_stage3.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss_joint\": train_loss3,\n",
    "            \"val_loss_att\": val_loss_att3,\n",
    "            \"val_loss_dev\": val_loss_dev3,\n",
    "            \"val_att_acc\": val_att_acc3,\n",
    "            \"val_dev_acc\": val_dev_acc3,\n",
    "            \"combined_metric\": combined_metric,\n",
    "            \"lr\": scheduler_stage3.get_last_lr()[0],\n",
    "        })\n",
    "\n",
    "        print(f\"  [Stage3] Train Loss: {train_loss3:.4f}\")\n",
    "        print(f\"  [Stage3] Val Att Loss: {val_loss_att3:.4f}, Val Att Acc: {val_att_acc3:.4f}\")\n",
    "        print(f\"  [Stage3] Val Dev Loss: {val_loss_dev3:.4f}, Val Dev Acc: {val_dev_acc3:.4f}\")\n",
    "        print(f\"  [Stage3] Combined metric: {combined_metric:.4f}\")\n",
    "\n",
    "        if combined_metric > best_combined_stage3 + 1e-4:\n",
    "            best_combined_stage3 = combined_metric\n",
    "            epochs_no_improve3 = 0\n",
    "            torch.save(model.state_dict(), best_stage3_model_path)\n",
    "            print(f\"  ✓ [Stage3] New best joint model saved (combined={combined_metric:.4f})\")\n",
    "        else:\n",
    "            epochs_no_improve3 += 1\n",
    "            print(f\"  [Stage3] No improvement for {epochs_no_improve3} epoch(s)\")\n",
    "\n",
    "        if epochs_no_improve3 >= PATIENCE_STAGE3:\n",
    "            print(\"\\n[Stage3] Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    pd.DataFrame(history_stage3).to_csv(REPORTS_DIR / \"multitask_stage3_joint_history.csv\", index=False)\n",
    "\n",
    "    # Load best Stage 3 model\n",
    "    model.load_state_dict(torch.load(best_stage3_model_path, map_location=device))\n",
    "\n",
    "    # Final TEST evaluation after joint fine-tune\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STAGE 3: Final TEST evaluation (joint fine-tuned model)\")\n",
    "    print(\"=\"*60)\n",
    "    test_loss_att3, test_loss_dev3, test_att_acc3, test_dev_acc3, y_att_true3, y_att_pred3, y_dev_true3, y_dev_pred3 = eval_both_heads(model, test_loader)\n",
    "    print(f\"[Stage3] Test Attack Loss: {test_loss_att3:.4f}, Test Attack Acc: {test_att_acc3:.4f}\")\n",
    "    print(f\"[Stage3] Test Device Loss: {test_loss_dev3:.4f}, Test Device Acc: {test_dev_acc3:.4f}\")\n",
    "\n",
    "    stage3_attack_report = classification_report(\n",
    "        y_att_true3,\n",
    "        y_att_pred3,\n",
    "        target_names=attack_names,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    print(\"\\n[Stage3] Attack head classification report (TEST):\")\n",
    "    print(stage3_attack_report)\n",
    "\n",
    "    dev_counts3 = pd.Series(y_dev_true3).value_counts()\n",
    "    top_k3 = min(20, len(dev_counts3))\n",
    "    top_dev_ids3 = dev_counts3.index[:top_k3]\n",
    "    mask_top3 = np.isin(y_dev_true3, top_dev_ids3)\n",
    "    y_dev_true_top3 = y_dev_true3[mask_top3]\n",
    "    y_dev_pred_top3 = y_dev_pred3[mask_top3]\n",
    "    top_dev_names3 = [device_label_mapping[str(i)] for i in top_dev_ids3]\n",
    "\n",
    "    stage3_device_report = classification_report(\n",
    "        y_dev_true_top3,\n",
    "        y_dev_pred_top3,\n",
    "        labels=top_dev_ids3,           # ensure labels size matches target_names\n",
    "        target_names=top_dev_names3,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    print(\"\\n[Stage3] Device head classification report (TEST) - Top 20 devices:\")\n",
    "    print(stage3_device_report)\n",
    "\n",
    "    with open(REPORTS_DIR / \"multitask_stage3_test_reports.txt\", \"w\") as f:\n",
    "        f.write(\"Attack head report (TEST):\\n\")\n",
    "        f.write(stage3_attack_report)\n",
    "        f.write(\"\\n\\nDevice head report (TEST) - Top 20 devices:\\n\")\n",
    "        f.write(stage3_device_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Multitask CNN+MLP staged training complete.\")\n",
    "print(\"=\"*60)\n",
    "print(\"Models saved in:\", MODELS_DIR)\n",
    "print(\"Histories and reports in:\", REPORTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
