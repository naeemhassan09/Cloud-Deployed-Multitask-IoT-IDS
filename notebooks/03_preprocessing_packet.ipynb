{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook: 03_preprocessing_packet.ipynb ---\n",
    "# Goal:\n",
    "# - Construct a training-ready multitask dataset from CIC IoT-IDAD 2024 packet-based CSV files\n",
    "# - Approximate size: ~3.6M rows (balanced across 8 attack families)\n",
    "# - Provide two supervised targets:\n",
    "#       * device_id   (encoded from device_mac)  → Device Identification\n",
    "#       * attack_id   (encoded from attack_label) → Intrusion Detection\n",
    "# - Export:\n",
    "#       * packets_train.csv\n",
    "#       * packets_val.csv\n",
    "#       * packets_test.csv\n",
    "#       * device_label_mapping.json\n",
    "#       * attack_label_mapping.json\n",
    "# - This dataset is used by the final multitask 1D-CNN + MLP model (shared backbone + two heads).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05421b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Environment setup & Configuration\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Optional progress bar for streaming read\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, **kwargs: x  # fallback no-op\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Assume notebook lives in <project_root>/notebooks/\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[0]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.load_data import IoTDatasetLoader\n",
    "\n",
    "\n",
    "\n",
    "# Raw CIC IoT-IDAD 2024 base directory\n",
    "BASE_PATH = Path(\n",
    "    \"/Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024\"\n",
    ")\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Target dataset size (chosen to fit in memory on 16 GB M1 while still being large)\n",
    "TARGET_TOTAL_ROWS = 3_600_000          # total rows across all 8 attack families\n",
    "USEABLE_FRACTION = 1.0                 # keep simple; can reduce if memory is tight\n",
    "FINAL_TARGET_ROWS = int(TARGET_TOTAL_ROWS * USEABLE_FRACTION)\n",
    "\n",
    "# Attack families of interest (benign + 7 attack categories)\n",
    "ATTACK_FAMILIES = [\n",
    "    \"benign\",\n",
    "    \"ddos\",\n",
    "    \"dos\",\n",
    "    \"mirai\",\n",
    "    \"recon\",\n",
    "    \"spoofing\",\n",
    "    \"web-based\",\n",
    "    \"brute force\",\n",
    "]\n",
    "\n",
    "# Aim for an approximately balanced dataset across these families\n",
    "TARGET_PER_ATTACK = FINAL_TARGET_ROWS // len(ATTACK_FAMILIES)\n",
    "\n",
    "# Chunk size for streaming CSVs\n",
    "CHUNK_SIZE = 200_000  # rows per chunk when reading large files\n",
    "\n",
    "# Output format (CSV used to avoid pyarrow dependency issues)\n",
    "USE_PARQUET = False  # kept for completeness; not used in current export\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db214e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Helper functions\n",
    "# ============================================================\n",
    "\n",
    "def infer_attack_label_from_filename(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Infer a coarse attack label from the CSV file path.\n",
    "    This is used to attach a stable 'attack_label' string before encoding.\n",
    "    Priority order is important to avoid misclassification (e.g. Mirai vs generic DoS).\n",
    "    \"\"\"\n",
    "    p = file_path.lower()\n",
    "\n",
    "    if \"benign\" in p:\n",
    "        return \"benign\"\n",
    "    if \"mirai\" in p:\n",
    "        return \"mirai\"\n",
    "    if \"ddos\" in p:\n",
    "        return \"ddos\"\n",
    "    if \"bruteforce\" in p or \"brute_force\" in p or \"brute-force\" in p:\n",
    "        return \"brute force\"\n",
    "    if \"spoof\" in p:\n",
    "        return \"spoofing\"\n",
    "    if \"recon\" in p:\n",
    "        return \"recon\"\n",
    "    if \"web\" in p:\n",
    "        return \"web-based\"\n",
    "    if \"dos\" in p:\n",
    "        return \"dos\"\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def memory_usage_mb(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute approximate memory usage of a DataFrame in MB.\"\"\"\n",
    "    return df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "\n",
    "def label_distribution(df: pd.DataFrame, label_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a small summary DataFrame with class counts and percentages\n",
    "    for a given label column (e.g. attack_label, attack_id).\n",
    "    \"\"\"\n",
    "    counts = df[label_col].value_counts().sort_index()\n",
    "    perc = counts / counts.sum() * 100.0\n",
    "    out = pd.DataFrame({\"count\": counts, \"percentage\": perc})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59382376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files found under BASE_PATH: 312\n",
      "Packet-based CSV files detected: 180\n",
      "Example packet-based files:\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/BenignTraffic/BenignTraffic.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/BenignTraffic/BenignTraffic1.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/BenignTraffic/BenignTraffic2.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/BenignTraffic/BenignTraffic3.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/BruteForce/DictionaryBruteForce/DictionaryBruteForce.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/DDoS/DDoS-ACK_Fragmentation/DDoS-ACK_Fragmentation.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/DDoS/DDoS-ACK_Fragmentation/DDoS-ACK_Fragmentation1.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/DDoS/DDoS-ACK_Fragmentation/DDoS-ACK_Fragmentation10.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/DDoS/DDoS-ACK_Fragmentation/DDoS-ACK_Fragmentation11.csv\n",
      "   /Users/naeemulhassan/naeem-p/CIC_IoT_IDAD_2024/CIC_IoT_IDAD_Dataset_2024/Device Identification_Anomaly Detection - Packet Based Features/DDoS/DDoS-ACK_Fragmentation/DDoS-ACK_Fragmentation12.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3. List packet-based files\n",
    "# ============================================================\n",
    "\n",
    "loader = IoTDatasetLoader(BASE_PATH)\n",
    "all_files = loader.list_files()\n",
    "print(f\"Total CSV files found under BASE_PATH: {len(all_files)}\")\n",
    "\n",
    "# Packet-based files are identified by naming convention used in CIC IoT-IDAD 2024\n",
    "packet_files = [\n",
    "    f for f in all_files\n",
    "    if \"device identification_Anomaly detection - packet based features\".lower() in f.lower()\n",
    "    or \"device identification_anomaly detection - packet based features\".lower() in f.lower()\n",
    "    or \"packet based features\".lower() in f.lower()\n",
    "]\n",
    "\n",
    "# Fallback heuristic if naming varies\n",
    "if len(packet_files) == 0:\n",
    "    packet_files = [f for f in all_files if \"packet\" in f.lower()]\n",
    "\n",
    "print(f\"Packet-based CSV files detected: {len(packet_files)}\")\n",
    "print(\"Example packet-based files:\")\n",
    "for f in packet_files[:10]:\n",
    "    print(\"  \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3914ae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target total rows: 3600000\n",
      "Target per attack family: 450000\n",
      "Attack families considered: ['benign', 'ddos', 'dos', 'mirai', 'recon', 'spoofing', 'web-based', 'brute force']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing packet CSV files: 100%|██████████| 180/180 [01:10<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attack counts collected (approx.):\n",
      "  benign      : 450,000\n",
      "  ddos        : 450,000\n",
      "  dos         : 450,000\n",
      "  mirai       : 450,000\n",
      "  recon       : 450,000\n",
      "  spoofing    : 450,000\n",
      "  web-based   : 206,067\n",
      "  brute force : 131,477\n",
      "\n",
      "Total selected rows (approx.): 3,037,544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Balanced sampling across attack families (streamed)\n",
    "# ============================================================\n",
    "\n",
    "# Track how many rows have been collected per attack family\n",
    "attack_counts = defaultdict(int)\n",
    "selected_chunks = []\n",
    "\n",
    "print(\"\\nTarget total rows:\", FINAL_TARGET_ROWS)\n",
    "print(\"Target per attack family:\", TARGET_PER_ATTACK)\n",
    "print(\"Attack families considered:\", ATTACK_FAMILIES)\n",
    "\n",
    "for fp in tqdm(packet_files, desc=\"Processing packet CSV files\"):\n",
    "    # Infer the coarse attack family for this file from its path\n",
    "    label = infer_attack_label_from_filename(fp)\n",
    "\n",
    "    # Skip files that are not part of the 8 target families\n",
    "    if label not in ATTACK_FAMILIES:\n",
    "        continue\n",
    "\n",
    "    # If this attack already reached its target quota, skip the file\n",
    "    if attack_counts[label] >= TARGET_PER_ATTACK:\n",
    "        continue\n",
    "\n",
    "    # Stream the CSV in chunks to avoid loading entire file into memory\n",
    "    for chunk in pd.read_csv(fp, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "        remaining_for_label = TARGET_PER_ATTACK - attack_counts[label]\n",
    "        if remaining_for_label <= 0:\n",
    "            break\n",
    "\n",
    "        # If the chunk is smaller than the remaining quota, take all rows\n",
    "        if len(chunk) <= remaining_for_label:\n",
    "            sampled_chunk = chunk\n",
    "        else:\n",
    "            # Otherwise randomly down-sample to exactly what is needed\n",
    "            sampled_chunk = chunk.sample(\n",
    "                n=remaining_for_label,\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "\n",
    "        # Attach multitask labels that we control:\n",
    "        sampled_chunk[\"attack_label\"] = label\n",
    "        sampled_chunk[\"source_file\"] = Path(fp).name\n",
    "\n",
    "        attack_counts[label] += len(sampled_chunk)\n",
    "        selected_chunks.append(sampled_chunk)\n",
    "\n",
    "        # Stop globally once total selected rows reach the target\n",
    "        if sum(attack_counts.values()) >= FINAL_TARGET_ROWS:\n",
    "            break\n",
    "\n",
    "    # Global early-stop check\n",
    "    if sum(attack_counts.values()) >= FINAL_TARGET_ROWS:\n",
    "        break\n",
    "\n",
    "print(\"\\nAttack counts collected (approx.):\")\n",
    "for k in ATTACK_FAMILIES:\n",
    "    print(f\"  {k:12s}: {attack_counts[k]:,}\")\n",
    "\n",
    "total_selected = sum(attack_counts.values())\n",
    "print(f\"\\nTotal selected rows (approx.): {total_selected:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8edcf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenated dataframe shape: (3037544, 137)\n",
      "Approx memory usage (MB): 5874.76\n",
      "\n",
      "Unique attack_label values in df_all: ['benign' 'brute force' 'ddos' 'dos' 'mirai' 'recon' 'spoofing'\n",
      " 'web-based']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Concatenate selected data and basic checks\n",
    "# ============================================================\n",
    "\n",
    "df_all = pd.concat(selected_chunks, ignore_index=True)\n",
    "print(\"\\nConcatenated dataframe shape:\", df_all.shape)\n",
    "print(\"Approx memory usage (MB):\", round(memory_usage_mb(df_all), 2))\n",
    "\n",
    "# Ensure device identifier column is present\n",
    "if \"device_mac\" not in df_all.columns:\n",
    "    raise ValueError(\"device_mac column not found in concatenated dataframe.\")\n",
    "\n",
    "# Check that attack_label is attached correctly\n",
    "print(\"\\nUnique attack_label values in df_all:\", df_all[\"attack_label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc9fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device label space size: 94\n",
      "Attack label space size: 8\n",
      "\n",
      "Sample device mapping (first 10):\n",
      "    0 -> 00:0c:29:03:b2:98\n",
      "    1 -> 00:0c:29:07:63:da\n",
      "    2 -> 00:0c:29:1c:55:4a\n",
      "    3 -> 00:0c:29:20:ab:ec\n",
      "    4 -> 00:0c:29:3e:f0:e0\n",
      "    5 -> 00:0c:29:c3:9b:8a\n",
      "    6 -> 00:0c:29:dd:6e:c7\n",
      "    7 -> 00:0c:29:f2:f3:74\n",
      "    8 -> 00:a3:d1:07:6f:03\n",
      "    9 -> 00:a3:d1:07:6f:05\n",
      "\n",
      "Attack mapping:\n",
      "   0 -> benign\n",
      "   1 -> brute force\n",
      "   2 -> ddos\n",
      "   3 -> dos\n",
      "   4 -> mirai\n",
      "   5 -> recon\n",
      "   6 -> spoofing\n",
      "   7 -> web-based\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. Label encoding for device_mac and attack_label\n",
    "# ============================================================\n",
    "\n",
    "# Device label encoding: device_mac → device_id\n",
    "device_labels = df_all[\"device_mac\"].astype(str).unique()\n",
    "device_labels_sorted = sorted(device_labels)\n",
    "\n",
    "device_to_id = {dev: i for i, dev in enumerate(device_labels_sorted)}\n",
    "id_to_device = {i: dev for dev, i in device_to_id.items()}\n",
    "\n",
    "df_all[\"device_id\"] = df_all[\"device_mac\"].astype(str).map(device_to_id)\n",
    "\n",
    "# Attack label encoding: attack_label → attack_id\n",
    "attack_labels_present = sorted(df_all[\"attack_label\"].astype(str).unique())\n",
    "attack_to_id = {lab: i for i, lab in enumerate(attack_labels_present)}\n",
    "id_to_attack = {i: lab for lab, i in attack_to_id.items()}\n",
    "\n",
    "df_all[\"attack_id\"] = df_all[\"attack_label\"].astype(str).map(attack_to_id)\n",
    "\n",
    "print(\"\\nDevice label space size:\", len(device_to_id))\n",
    "print(\"Attack label space size:\", len(attack_to_id))\n",
    "\n",
    "print(\"\\nSample device mapping (first 10):\")\n",
    "for i, (dev, idx) in enumerate(device_to_id.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  {idx:3d} -> {dev}\")\n",
    "\n",
    "print(\"\\nAttack mapping:\")\n",
    "for lab, idx in attack_to_id.items():\n",
    "    print(f\"  {idx:2d} -> {lab}\")\n",
    "\n",
    "# Persist label mappings for downstream training / deployment\n",
    "with open(PROCESSED_DIR / \"device_label_mapping.json\", \"w\") as f:\n",
    "    json.dump({\"device_to_id\": device_to_id, \"id_to_device\": id_to_device}, f, indent=2)\n",
    "\n",
    "with open(PROCESSED_DIR / \"attack_label_mapping.json\", \"w\") as f:\n",
    "    json.dump({\"attack_to_id\": attack_to_id, \"id_to_attack\": id_to_attack}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbad4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split sizes (stratified by attack_id):\n",
      "  Train: 2,126,280 rows\n",
      "  Val  : 455,632 rows\n",
      "  Test : 455,632 rows\n",
      "\n",
      "Attack distribution in Train (attack_label):\n",
      "               count  percentage\n",
      "attack_label                    \n",
      "benign        315000   14.814606\n",
      "brute force    92034    4.328405\n",
      "ddos          315000   14.814606\n",
      "dos           315000   14.814606\n",
      "mirai         315000   14.814606\n",
      "recon         315000   14.814606\n",
      "spoofing      315000   14.814606\n",
      "web-based     144246    6.783961\n",
      "\n",
      "Attack distribution in Val (attack_label):\n",
      "              count  percentage\n",
      "attack_label                   \n",
      "benign        67500   14.814587\n",
      "brute force   19721    4.328274\n",
      "ddos          67500   14.814587\n",
      "dos           67500   14.814587\n",
      "mirai         67500   14.814587\n",
      "recon         67500   14.814587\n",
      "spoofing      67500   14.814587\n",
      "web-based     30911    6.784203\n",
      "\n",
      "Attack distribution in Test (attack_label):\n",
      "              count  percentage\n",
      "attack_label                   \n",
      "benign        67500   14.814587\n",
      "brute force   19722    4.328493\n",
      "ddos          67500   14.814587\n",
      "dos           67500   14.814587\n",
      "mirai         67500   14.814587\n",
      "recon         67500   14.814587\n",
      "spoofing      67500   14.814587\n",
      "web-based     30910    6.783984\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. Train / validation / test split (stratified by attack_id)\n",
    "# ============================================================\n",
    "\n",
    "# First split: Train (70%) vs Temp (30%), stratified by attack_id\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df_all[\"attack_id\"],\n",
    ")\n",
    "\n",
    "# Second split: Temp → Validation (15%) + Test (15%)\n",
    "# Since temp is 30%, splitting 50/50 gives 15/15 overall.\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=temp_df[\"attack_id\"],\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes (stratified by attack_id):\")\n",
    "print(f\"  Train: {len(train_df):,} rows\")\n",
    "print(f\"  Val  : {len(val_df):,} rows\")\n",
    "print(f\"  Test : {len(test_df):,} rows\")\n",
    "\n",
    "print(\"\\nAttack distribution in Train (attack_label):\")\n",
    "print(label_distribution(train_df, \"attack_label\"))\n",
    "\n",
    "print(\"\\nAttack distribution in Val (attack_label):\")\n",
    "print(label_distribution(val_df, \"attack_label\"))\n",
    "\n",
    "print(\"\\nAttack distribution in Test (attack_label):\")\n",
    "print(label_distribution(test_df, \"attack_label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7184ff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved processed CSV files to /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed:\n",
      "   packets_train.csv\n",
      "   packets_val.csv\n",
      "   packets_test.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. Save processed datasets (CSV format)\n",
    "# ============================================================\n",
    "\n",
    "train_path = PROCESSED_DIR / \"packets_train.csv\"\n",
    "val_path   = PROCESSED_DIR / \"packets_val.csv\"\n",
    "test_path  = PROCESSED_DIR / \"packets_test.csv\"\n",
    "\n",
    "# CSV is explicit to avoid environment-specific parquet issues\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved processed CSV files to {PROCESSED_DIR}:\")\n",
    "print(\"  \", train_path.name)\n",
    "print(\"  \", val_path.name)\n",
    "print(\"  \", test_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "becad84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ PREPROCESSING SUMMARY ================\n",
      "- Total selected rows for multitask dataset : 3,037,544\n",
      "- Attack families included and per-class counts (df_all):\n",
      "    benign      : 450,000\n",
      "    brute force : 131,477\n",
      "    ddos        : 450,000\n",
      "    dos         : 450,000\n",
      "    mirai       : 450,000\n",
      "    recon       : 450,000\n",
      "    spoofing    : 450,000\n",
      "    web-based   : 206,067\n",
      "- Number of device classes (device_id)      : 94\n",
      "- Train / Val / Test shapes (rows, columns):\n",
      "    Train: (2126280, 139)\n",
      "    Val  : (455632, 139)\n",
      "    Test : (455632, 139)\n",
      "- Label mappings saved to:\n",
      "    /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed/device_label_mapping.json\n",
      "    /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed/attack_label_mapping.json\n",
      "- Datasets saved to:\n",
      "    /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed/packets_train.csv\n",
      "    /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed/packets_val.csv\n",
      "    /Users/naeemulhassan/naeem-p/Cloud-Deployed-Multitask-IoT-IDS/data/processed/packets_test.csv\n",
      "- This preprocessed dataset is used by the final multitask\n",
      "  1D-CNN + MLP model with a shared backbone and two heads\n",
      "  (attack_id and device_id).\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. Summary (for report / supervisor / methodology section)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n================ PREPROCESSING SUMMARY ================\")\n",
    "print(f\"- Total selected rows for multitask dataset : {len(df_all):,}\")\n",
    "print(\"- Attack families included and per-class counts (df_all):\")\n",
    "for lab in attack_labels_present:\n",
    "    cnt = int((df_all[\"attack_label\"] == lab).sum())\n",
    "    print(f\"    {lab:12s}: {cnt:,}\")\n",
    "print(f\"- Number of device classes (device_id)      : {len(device_to_id)}\")\n",
    "print(\"- Train / Val / Test shapes (rows, columns):\")\n",
    "print(f\"    Train: {train_df.shape}\")\n",
    "print(f\"    Val  : {val_df.shape}\")\n",
    "print(f\"    Test : {test_df.shape}\")\n",
    "print(\"- Label mappings saved to:\")\n",
    "print(f\"    {PROCESSED_DIR / 'device_label_mapping.json'}\")\n",
    "print(f\"    {PROCESSED_DIR / 'attack_label_mapping.json'}\")\n",
    "print(\"- Datasets saved to:\")\n",
    "print(f\"    {train_path}\")\n",
    "print(f\"    {val_path}\")\n",
    "print(f\"    {test_path}\")\n",
    "print(\"- This preprocessed dataset is used by the final multitask\")\n",
    "print(\"  1D-CNN + MLP model with a shared backbone and two heads\")\n",
    "print(\"  (attack_id and device_id).\")\n",
    "print(\"=======================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
